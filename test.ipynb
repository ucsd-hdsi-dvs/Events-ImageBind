{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import torch\n",
    "from models import imagebind_model\n",
    "from models.imagebind_model import ModalityType\n",
    "\n",
    "text_list=[\"A dog.\", \"A car\", \"A bird\"]\n",
    "image_paths=[\".assets/dog_image.jpg\", \".assets/car_image.jpg\", \".assets/bird_image.jpg\"]\n",
    "audio_paths=[\".assets/dog_audio.wav\", \".assets/car_audio.wav\", \".assets/bird_audio.wav\"]\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Load data\n",
    "inputs = {\n",
    "    ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "    ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n",
    "    ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(inputs)\n",
    "\n",
    "print(\n",
    "    \"Vision x Text: \",\n",
    "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    ")\n",
    "print(\n",
    "    \"Audio x Text: \",\n",
    "    torch.softmax(embeddings[ModalityType.AUDIO] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    ")\n",
    "print(\n",
    "    \"Vision x Audio: \",\n",
    "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.AUDIO].T, dim=-1),\n",
    ")\n",
    "\n",
    "# Expected output:\n",
    "#\n",
    "# Vision x Text:\n",
    "# tensor([[9.9761e-01, 2.3694e-03, 1.8612e-05],\n",
    "#         [3.3836e-05, 9.9994e-01, 2.4118e-05],\n",
    "#         [4.7997e-05, 1.3496e-02, 9.8646e-01]])\n",
    "#\n",
    "# Audio x Text:\n",
    "# tensor([[1., 0., 0.],\n",
    "#         [0., 1., 0.],\n",
    "#         [0., 0., 1.]])\n",
    "#\n",
    "# Vision x Audio:\n",
    "# tensor([[0.8070, 0.1088, 0.0842],\n",
    "#         [0.1036, 0.7884, 0.1079],\n",
    "#         [0.0018, 0.0022, 0.9960]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.AUDIO].T, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.multimodal_preprocessors import PatchEmbedGeneric,PadIm2Video\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "class PatchEmbedGeneric(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchEmbed from Hydra\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, proj_stem, norm_layer: Optional[nn.Module] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        if len(proj_stem) > 1:\n",
    "            self.proj = nn.Sequential(*proj_stem)\n",
    "        else:\n",
    "            # Special case to be able to load pre-trained models that were\n",
    "            # trained with a standard stem\n",
    "            self.proj = proj_stem[0]\n",
    "        self.norm_layer = norm_layer\n",
    "\n",
    "    def get_patch_layout(self, img_size):\n",
    "        with torch.no_grad():\n",
    "            dummy_img = torch.zeros(\n",
    "                [\n",
    "                    1,\n",
    "                ]\n",
    "                + img_size\n",
    "            )\n",
    "            print(dummy_img.shape)\n",
    "            dummy_out = self.proj(dummy_img)\n",
    "        print(dummy_out.shape)\n",
    "        embed_dim = dummy_out.shape[1]\n",
    "        patches_layout = tuple(dummy_out.shape[2:])\n",
    "        num_patches = np.prod(patches_layout)\n",
    "        return patches_layout, num_patches, embed_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = self.proj(x)\n",
    "        print(x.shape)\n",
    "        # B C (T) H W -> B (T)HW C\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        print(x.shape)\n",
    "        if self.norm_layer is not None:\n",
    "            x = self.norm_layer(x)\n",
    "        return x\n",
    "    \n",
    "kernel_size=(2, 14, 14)\n",
    "vision_embed_dim=1024\n",
    "proj_stem=[\n",
    "                PadIm2Video(pad_type=\"repeat\", ntimes=2),\n",
    "                nn.Conv3d(\n",
    "                    in_channels=3,\n",
    "                    kernel_size=kernel_size,\n",
    "                    out_channels=vision_embed_dim,\n",
    "                    stride=kernel_size,\n",
    "                    bias=False,\n",
    "                )\n",
    "]\n",
    "PatchEmbedGeneric(proj_stem,None).get_patch_layout([3, 2,224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.helpers import VerboseNNModule\n",
    "from typing import Tuple, Optional, Callable\n",
    "from models.helpers import (EinOpsRearrange, LearnableLogitScaling, Normalize,\n",
    "                            SelectElement, SelectEOSAndProject)\n",
    "from models.multimodal_preprocessors import (AudioPreprocessor,\n",
    "                                             IMUPreprocessor, PadIm2Video,\n",
    "                                             PatchEmbedGeneric,\n",
    "                                             RGBDTPreprocessor,\n",
    "                                             SpatioTemporalPosEmbeddingHelper,\n",
    "                                             TextPreprocessor,\n",
    "                                             ThermalPreprocessor)\n",
    "from models.transformer import MultiheadAttention, SimpleTransformer\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from functools import partial\n",
    "from types import SimpleNamespace\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RGBDTPreprocessor(VerboseNNModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rgbt_stem: PatchEmbedGeneric,\n",
    "        depth_stem: Optional[PatchEmbedGeneric],\n",
    "        img_size: Tuple = (3, 224, 224),\n",
    "        num_cls_tokens: int = 1,\n",
    "        pos_embed_fn: Optional[Callable] = None,\n",
    "        use_type_embed: bool = False,\n",
    "        init_param_style: str = \"openclip\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        stem = rgbt_stem if rgbt_stem is not None else depth_stem\n",
    "        (\n",
    "            self.patches_layout,\n",
    "            self.num_patches,\n",
    "            self.embed_dim,\n",
    "        ) = stem.get_patch_layout(img_size)\n",
    "        self.rgbt_stem = rgbt_stem\n",
    "        self.depth_stem = depth_stem\n",
    "        self.use_pos_embed = pos_embed_fn is not None\n",
    "        self.use_type_embed = use_type_embed\n",
    "        self.num_cls_tokens = num_cls_tokens\n",
    "\n",
    "        if self.use_pos_embed:\n",
    "            self.pos_embedding_helper = pos_embed_fn(\n",
    "                patches_layout=self.patches_layout,\n",
    "                num_cls_tokens=num_cls_tokens,\n",
    "                num_patches=self.num_patches,\n",
    "                embed_dim=self.embed_dim,\n",
    "            )\n",
    "        if self.num_cls_tokens > 0:\n",
    "            self.cls_token = nn.Parameter(\n",
    "                torch.zeros(1, self.num_cls_tokens, self.embed_dim)\n",
    "            )\n",
    "        if self.use_type_embed:\n",
    "            self.type_embed = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "\n",
    "        self.init_parameters(init_param_style)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_parameters(self, init_param_style):\n",
    "        if init_param_style == \"openclip\":\n",
    "            # OpenCLIP style initialization\n",
    "            scale = self.embed_dim**-0.5\n",
    "            if self.use_pos_embed:\n",
    "                nn.init.normal_(self.pos_embedding_helper.pos_embed)\n",
    "                self.pos_embedding_helper.pos_embed *= scale\n",
    "\n",
    "            if self.num_cls_tokens > 0:\n",
    "                nn.init.normal_(self.cls_token)\n",
    "                self.cls_token *= scale\n",
    "        elif init_param_style == \"vit\":\n",
    "            self.cls_token.data.fill_(0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown init {init_param_style}\")\n",
    "\n",
    "        if self.use_type_embed:\n",
    "            nn.init.normal_(self.type_embed)\n",
    "\n",
    "    def tokenize_input_and_cls_pos(self, input, stem, mask):\n",
    "        # tokens is of shape B x L x D\n",
    "        tokens = stem(input)\n",
    "        assert tokens.ndim == 3\n",
    "        assert tokens.shape[2] == self.embed_dim\n",
    "        B = tokens.shape[0]\n",
    "        if self.num_cls_tokens > 0:\n",
    "            class_tokens = self.cls_token.expand(\n",
    "                B, -1, -1\n",
    "            )  # stole class_tokens impl from Phil Wang, thanks\n",
    "            tokens = torch.cat((class_tokens, tokens), dim=1)\n",
    "        if self.use_pos_embed:\n",
    "            pos_embed = self.pos_embedding_helper.get_pos_embedding(input, tokens)\n",
    "            tokens = tokens + pos_embed\n",
    "        if self.use_type_embed:\n",
    "            tokens = tokens + self.type_embed.expand(B, -1, -1)\n",
    "        return tokens\n",
    "\n",
    "    def forward(self, vision=None, depth=None, patch_mask=None):\n",
    "        if patch_mask is not None:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        if vision is not None:\n",
    "            vision_tokens = self.tokenize_input_and_cls_pos(\n",
    "                vision, self.rgbt_stem, patch_mask\n",
    "            )\n",
    "\n",
    "        if depth is not None:\n",
    "            depth_tokens = self.tokenize_input_and_cls_pos(\n",
    "                depth, self.depth_stem, patch_mask\n",
    "            )\n",
    "\n",
    "        # aggregate tokens\n",
    "        if vision is not None and depth is not None:\n",
    "            final_tokens = vision_tokens + depth_tokens\n",
    "        else:\n",
    "            final_tokens = vision_tokens if vision is not None else depth_tokens\n",
    "        return_dict = {\n",
    "            \"trunk\": {\n",
    "                \"tokens\": final_tokens,\n",
    "            },\n",
    "            \"head\": {},\n",
    "        }\n",
    "        return return_dict\n",
    "\n",
    "rgbt_stem = PatchEmbedGeneric(\n",
    "            proj_stem=[\n",
    "                PadIm2Video(pad_type=\"repeat\", ntimes=2),\n",
    "                nn.Conv3d(\n",
    "                    in_channels=3,\n",
    "                    kernel_size=kernel_size,\n",
    "                    out_channels=vision_embed_dim,\n",
    "                    stride=kernel_size,\n",
    "                    bias=False,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "rgbt_preprocessor = RGBDTPreprocessor(\n",
    "            img_size=[3, 2, 224, 224],\n",
    "            num_cls_tokens=1,\n",
    "            pos_embed_fn=partial(SpatioTemporalPosEmbeddingHelper, learnable=True),\n",
    "            rgbt_stem=rgbt_stem,\n",
    "            depth_stem=None,\n",
    "        )\n",
    "\n",
    "rgbt_preprocessor(dummy_img[0])['trunk']['tokens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/imagebind/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models.imagebind_model import ImageBindModel\n",
    "imageBind=ImageBindModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([1, 1, 224, 224])\n",
      "trunk_inputs shape torch.Size([1, 197, 768])\n",
      "after trunk shape torch.Size([1, 197, 768])\n",
      "after head shape torch.Size([1, 768])\n",
      "postprocessor shape torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dummy_img = torch.zeros(\n",
    "                [\n",
    "                    1,\n",
    "                ]\n",
    "                + [1, 224, 224]\n",
    "            )\n",
    "imageBind.layer_shapes(dummy_img,modality_type=\"thermal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "path='.checkpoints/imagebind_huge.pth'\n",
    "state_dict = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['modality_heads.thermal.2.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modality_preprocessors.thermal.cls_token',\n",
       " 'modality_preprocessors.thermal.rgbt_stem.proj.weight',\n",
       " 'modality_preprocessors.thermal.rgbt_stem.norm_layer.weight',\n",
       " 'modality_preprocessors.thermal.rgbt_stem.norm_layer.bias',\n",
       " 'modality_preprocessors.thermal.pos_embedding_helper.pos_embed',\n",
       " 'modality_trunks.thermal.blocks.0.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.0.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.0.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.0.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.0.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.0.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.0.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.0.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.0.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.0.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.0.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.0.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.0.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.0.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.1.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.1.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.1.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.1.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.1.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.1.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.1.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.1.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.1.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.1.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.1.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.1.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.1.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.1.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.2.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.2.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.2.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.2.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.2.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.2.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.2.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.2.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.2.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.2.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.2.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.2.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.2.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.2.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.3.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.3.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.3.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.3.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.3.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.3.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.3.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.3.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.3.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.3.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.3.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.3.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.3.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.3.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.4.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.4.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.4.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.4.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.4.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.4.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.4.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.4.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.4.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.4.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.4.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.4.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.4.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.4.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.5.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.5.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.5.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.5.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.5.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.5.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.5.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.5.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.5.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.5.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.5.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.5.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.5.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.5.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.6.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.6.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.6.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.6.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.6.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.6.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.6.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.6.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.6.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.6.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.6.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.6.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.6.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.6.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.7.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.7.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.7.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.7.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.7.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.7.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.7.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.7.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.7.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.7.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.7.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.7.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.7.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.7.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.8.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.8.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.8.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.8.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.8.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.8.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.8.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.8.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.8.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.8.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.8.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.8.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.8.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.8.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.9.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.9.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.9.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.9.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.9.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.9.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.9.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.9.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.9.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.9.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.9.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.9.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.9.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.9.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.10.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.10.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.10.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.10.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.10.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.10.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.10.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.10.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.10.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.10.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.10.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.10.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.10.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.10.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.11.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.11.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.11.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.11.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.11.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.11.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.11.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.11.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.11.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.11.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.11.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.11.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.11.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.11.norm_2.bias',\n",
       " 'modality_heads.thermal.0.weight',\n",
       " 'modality_heads.thermal.0.bias',\n",
       " 'modality_heads.thermal.2.weight',\n",
       " 'modality_postprocessors.thermal.1.log_logit_scale']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[keys for keys in state_dict.keys() if 'thermal' in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modality_preprocessors.thermal.cls_token',\n",
       " 'modality_preprocessors.thermal.rgbt_stem.proj.weight',\n",
       " 'modality_preprocessors.thermal.rgbt_stem.norm_layer.weight',\n",
       " 'modality_preprocessors.thermal.rgbt_stem.norm_layer.bias',\n",
       " 'modality_preprocessors.thermal.pos_embedding_helper.pos_embed']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[key for key in state_dict.keys() if key.startswith(\"modality_preprocessors.thermal\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.events.EventModel at 0x7f975b4e1220>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.events import EventModel\n",
    "e=EventModel()\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[ 1.6901e-03,  2.0244e-02,  2.6660e-02,  5.9328e-02,  3.1017e-02,\n",
       "          -1.8562e-02,  1.1770e-02,  5.9572e-02, -7.6173e-02,  5.7392e-03,\n",
       "           4.9086e-02,  1.0775e-02,  4.0713e-02, -4.3928e-02, -3.2772e-02,\n",
       "           2.1218e-02, -3.3414e-02,  3.4800e-02,  3.6663e-02,  2.5046e-05,\n",
       "          -6.2629e-03, -7.0296e-02, -5.2900e-02,  2.8473e-04, -1.8450e-02,\n",
       "          -4.5848e-02,  8.9453e-03, -5.9607e-03,  1.8311e-02,  1.0183e-02,\n",
       "           4.7936e-02,  2.5173e-02,  3.6072e-03,  5.9513e-02,  1.7499e-02,\n",
       "           7.0098e-02,  5.5672e-03,  8.9034e-04,  7.3013e-02, -1.8981e-02,\n",
       "          -5.8576e-03,  5.1198e-03,  3.6699e-02,  1.9843e-02,  2.8855e-02,\n",
       "           1.4209e-02,  7.4288e-03, -3.2217e-02,  8.0838e-02, -2.0358e-02,\n",
       "          -2.2422e-02, -3.4644e-02, -1.6200e-02,  2.9877e-02, -2.0436e-02,\n",
       "           4.5344e-02, -1.2675e-02,  3.2115e-02, -2.5863e-02, -2.4318e-02,\n",
       "          -8.2837e-04,  2.7650e-02, -6.2197e-02,  3.4397e-03, -2.9415e-02,\n",
       "          -8.4561e-02, -1.4685e-02, -2.7768e-02, -6.2531e-02,  2.8095e-02,\n",
       "          -1.3836e-02, -1.8076e-02, -2.8084e-02,  4.4022e-02,  5.7961e-03,\n",
       "          -1.5763e-03,  5.3547e-02,  2.5120e-03,  6.9163e-02, -3.3955e-02,\n",
       "          -2.5195e-02,  3.3691e-02,  3.3457e-03, -4.2246e-02,  7.6265e-03,\n",
       "           3.8203e-02,  4.7995e-02, -6.2057e-03, -3.3564e-02,  1.5066e-02,\n",
       "           9.0364e-02,  1.0330e-01,  4.8033e-02, -1.8326e-02, -1.1244e-02,\n",
       "           4.8529e-03, -1.9543e-02, -1.2146e-02, -7.7521e-03, -3.2521e-02,\n",
       "           1.8278e-02, -3.7352e-02,  1.7655e-03,  4.8990e-03, -3.9552e-02,\n",
       "           4.5741e-03, -8.1741e-03, -3.5163e-02, -8.6424e-03,  2.4896e-02,\n",
       "           1.0849e-02,  2.0549e-02, -5.8706e-02,  4.6507e-02,  8.0650e-02,\n",
       "           6.5753e-02, -3.0569e-02,  4.4671e-02,  5.2230e-03,  2.3189e-02,\n",
       "           1.1198e-02,  1.2862e-02, -1.0550e-02,  5.9622e-02, -3.5307e-05,\n",
       "          -1.9523e-02, -1.3899e-02,  2.2564e-02, -1.1505e-02,  4.5265e-02,\n",
       "           3.5822e-02,  7.0570e-02, -2.1362e-03,  2.8674e-02, -2.1527e-02,\n",
       "          -7.8797e-03, -3.1861e-03, -2.7913e-02, -4.2741e-02,  1.1246e-02,\n",
       "          -1.4744e-03,  3.0421e-02,  4.1376e-02, -2.9488e-02, -1.4218e-02,\n",
       "          -3.1494e-02, -3.4486e-02, -3.3528e-04, -5.8901e-03, -1.1337e-02,\n",
       "           6.3284e-02,  3.4131e-02, -6.4371e-02, -3.1828e-03,  4.9520e-02,\n",
       "           2.8135e-02,  3.3472e-02,  6.3996e-02, -3.6980e-02, -5.0270e-02,\n",
       "           6.9554e-02,  2.5796e-03, -4.2771e-02, -3.0426e-04, -2.6159e-02,\n",
       "           3.4039e-02,  2.1481e-02, -3.7333e-02,  1.0398e-02,  1.6319e-02,\n",
       "           3.2354e-02, -4.1929e-02,  1.6784e-02,  9.5320e-03,  3.4200e-02,\n",
       "          -6.7337e-02,  4.1303e-02, -3.6834e-02, -1.2353e-02,  4.5845e-02,\n",
       "           3.5424e-02, -3.3273e-02,  1.0675e-02, -3.0196e-03,  2.4961e-02,\n",
       "           1.1907e-02, -2.4249e-02, -5.9536e-03,  9.3554e-03,  2.4587e-02,\n",
       "           2.8619e-02, -3.5235e-02,  4.0433e-02, -2.7141e-02, -1.0061e-03,\n",
       "          -2.9600e-03, -3.1096e-02,  3.7871e-02,  6.9553e-02,  7.9566e-03,\n",
       "           8.9300e-03,  4.1145e-02,  3.2909e-02, -4.1591e-02,  1.8598e-02,\n",
       "           2.9429e-02, -1.3847e-02,  2.0752e-02, -2.5669e-02,  2.4855e-03,\n",
       "          -1.1604e-02, -1.7706e-02, -7.0722e-03,  2.7284e-03, -2.0014e-02,\n",
       "           3.5511e-02,  1.3136e-03,  2.1657e-03, -1.4325e-02,  5.6201e-02,\n",
       "           4.3614e-02, -6.2908e-02,  5.5975e-02,  1.7675e-02,  4.9105e-02,\n",
       "          -1.1438e-02, -5.1215e-03, -2.9885e-02, -4.5875e-02, -1.8835e-02,\n",
       "           1.1336e-02, -2.6614e-02,  4.9690e-03,  4.8766e-02,  1.9314e-02,\n",
       "           5.3496e-02, -6.5708e-02, -2.8449e-02, -5.9912e-02, -2.3984e-02,\n",
       "           8.3026e-03,  2.9644e-02,  3.7633e-02, -1.4243e-02,  7.2658e-02,\n",
       "          -5.2853e-02,  1.5758e-02,  2.0977e-02,  2.5362e-02,  3.3583e-03,\n",
       "           5.9824e-02, -1.3947e-02, -1.0589e-03,  8.8533e-03,  2.6071e-02,\n",
       "          -2.5788e-02, -1.5799e-02, -7.1496e-03, -3.1046e-02, -5.4898e-03,\n",
       "          -6.2426e-02,  6.9147e-02,  5.4556e-02,  4.0906e-02, -1.2103e-02,\n",
       "           7.8463e-03, -2.6250e-02,  7.1943e-03,  4.0694e-02,  2.7989e-02,\n",
       "           6.7449e-03, -1.3529e-02, -3.5236e-02,  3.0140e-02,  3.7317e-02,\n",
       "           6.5226e-02,  1.3790e-02,  4.2505e-02,  5.0997e-02, -3.3082e-03,\n",
       "           3.1619e-02,  1.4557e-02, -7.0290e-03, -2.8328e-03,  1.0400e-02,\n",
       "          -4.9240e-03, -6.8299e-02,  3.8970e-03, -2.5631e-02,  1.3567e-02,\n",
       "           4.0121e-02,  4.2993e-02, -1.1834e-02,  8.7654e-03, -8.4850e-03,\n",
       "           7.5053e-02, -7.3827e-03,  1.1547e-02, -3.8752e-02, -1.7971e-02,\n",
       "           1.9019e-02, -1.2170e-02,  2.3198e-02,  7.1675e-03, -2.5105e-02,\n",
       "           4.5894e-02, -2.2776e-03,  3.8530e-02,  3.1755e-03, -1.3996e-02,\n",
       "           1.0483e-02,  3.3678e-03,  1.7655e-02, -1.1668e-01,  2.2107e-02,\n",
       "           2.1532e-02,  1.9829e-02, -5.9551e-03, -5.7875e-02, -4.2360e-02,\n",
       "          -1.8019e-02, -5.6051e-02, -4.5918e-02, -1.4393e-02, -3.5070e-02,\n",
       "          -4.0805e-02, -5.6556e-02,  5.0803e-02, -8.1611e-03,  9.5964e-04,\n",
       "          -1.0372e-02, -6.8253e-02,  5.4493e-02,  9.1972e-03, -1.3119e-03,\n",
       "          -6.8794e-02,  7.8860e-02, -7.0786e-03,  8.1038e-03,  1.4258e-02,\n",
       "          -8.4942e-04,  1.0859e-02,  1.1259e-02, -1.5091e-02, -1.7777e-02,\n",
       "           4.4874e-02, -1.4558e-02,  1.4490e-02,  6.1500e-03,  1.8439e-02,\n",
       "          -4.2051e-03, -8.5804e-02, -6.8820e-02, -4.6455e-02,  9.4333e-02,\n",
       "           5.0228e-02, -2.9532e-02,  2.8260e-02, -5.5800e-03,  9.1409e-03,\n",
       "           1.5313e-02,  1.5086e-03,  3.0451e-02, -5.8549e-02,  2.5625e-02,\n",
       "          -8.1153e-02,  1.4769e-02,  1.2027e-02,  1.9118e-02, -2.5774e-02,\n",
       "          -6.2410e-02,  5.9439e-02,  9.7922e-03,  2.7097e-03,  3.6330e-02,\n",
       "           2.5742e-02, -6.4990e-02,  6.1169e-02,  8.1808e-02, -1.1430e-02,\n",
       "           2.1642e-02,  6.1431e-02, -4.9584e-02, -6.2056e-02, -1.5541e-02,\n",
       "           5.5180e-02, -1.1031e-02, -3.0085e-03, -7.7243e-02, -4.8688e-02,\n",
       "           2.8009e-02, -1.4239e-02, -8.2117e-03,  4.6640e-02,  2.4973e-02,\n",
       "          -7.4961e-03, -2.2582e-02,  2.8532e-02,  7.4765e-03,  3.3594e-02,\n",
       "           1.8661e-02, -8.6980e-04, -8.7178e-03, -4.9153e-02, -2.9644e-02,\n",
       "           2.7664e-02,  4.8196e-03, -7.9867e-02,  4.2835e-02, -3.4685e-02,\n",
       "          -1.4244e-02, -1.2275e-02, -1.9677e-03, -7.3211e-02,  4.1760e-02,\n",
       "          -1.0991e-01, -1.3533e-02, -9.1565e-02,  4.2708e-03, -1.9985e-02,\n",
       "           3.0559e-02,  4.9675e-03, -2.1002e-02, -3.0442e-02,  2.0009e-02,\n",
       "           1.4235e-02, -2.0192e-02, -8.4630e-02,  3.7122e-02,  4.2605e-03,\n",
       "           2.9907e-02,  1.5306e-02, -4.3595e-03,  1.2545e-02,  1.4285e-02,\n",
       "          -5.9868e-02, -2.4585e-03,  6.7937e-02, -1.7703e-02,  1.1480e-02,\n",
       "           1.1060e-02, -2.6550e-02,  3.3802e-02,  3.7034e-02,  1.9601e-02,\n",
       "           6.0629e-04, -2.2018e-02, -7.9057e-03, -1.3072e-02,  5.8354e-02,\n",
       "          -3.5756e-02, -4.7227e-02, -7.5533e-03,  2.8095e-02, -5.7686e-02,\n",
       "           3.2393e-02,  1.4295e-04, -9.5056e-03,  5.8737e-03, -7.1507e-02,\n",
       "          -3.7142e-03,  2.0819e-02, -1.3668e-02,  1.7279e-02, -7.3306e-04,\n",
       "           8.0633e-02,  3.3087e-02, -3.1989e-04,  6.7128e-02, -5.3676e-03,\n",
       "           1.5998e-02,  2.6046e-02, -3.8522e-02, -1.9241e-02, -4.8037e-03,\n",
       "          -4.8503e-02,  2.1814e-02,  9.2001e-02,  3.5380e-02,  4.4476e-02,\n",
       "          -2.9634e-03,  7.2713e-02,  2.3618e-02,  1.2864e-02,  6.3902e-02,\n",
       "          -2.3104e-02,  2.1374e-02, -4.0103e-03, -6.7447e-03,  4.0836e-02,\n",
       "          -5.4427e-02, -3.6238e-02, -3.9927e-02, -9.7789e-02, -9.5667e-03,\n",
       "          -3.2980e-02, -5.8764e-02,  5.8404e-02, -7.1291e-02,  3.8254e-02,\n",
       "          -4.9907e-02,  1.7587e-02,  6.1026e-03,  2.2741e-02, -4.1345e-02,\n",
       "          -7.7887e-04, -1.2410e-02,  4.7789e-02, -4.7603e-02, -6.6969e-03,\n",
       "          -1.5889e-02,  3.0845e-03, -2.6642e-03,  1.5736e-02,  1.9587e-02,\n",
       "          -8.3717e-03,  3.0935e-02, -2.4967e-02, -4.2605e-02, -4.0067e-02,\n",
       "          -6.6783e-03,  8.5213e-03,  1.4502e-04, -2.2595e-02, -3.6530e-02,\n",
       "          -1.3217e-03,  1.0386e-03, -1.3588e-02,  8.5146e-04,  1.2440e-02,\n",
       "           3.8352e-03, -5.1085e-02,  2.4942e-02,  5.4924e-02, -1.3153e-02,\n",
       "          -5.4276e-02,  2.6516e-02,  2.8219e-02,  1.7405e-02,  3.2477e-02,\n",
       "           5.8570e-02, -1.9291e-02,  6.2069e-03,  2.8457e-03, -2.2673e-02,\n",
       "           3.0470e-02,  3.4065e-02, -3.4298e-02, -1.1340e-02,  5.0298e-03,\n",
       "          -1.8212e-02,  2.0462e-02, -2.5056e-02,  4.5023e-03,  2.4118e-03,\n",
       "           1.9319e-02,  3.8434e-02, -4.8709e-02,  1.5852e-02,  1.1074e-02,\n",
       "           2.4934e-02,  1.4100e-02,  4.7697e-02,  1.4269e-03,  6.2746e-02,\n",
       "           2.5034e-03, -9.2025e-03,  1.6545e-02, -3.8247e-02,  4.3937e-02,\n",
       "          -9.1368e-03, -3.9167e-02, -5.8088e-02, -2.9463e-02, -9.7704e-03,\n",
       "           1.7396e-02,  1.9587e-02, -3.8270e-02,  2.1151e-03,  4.8642e-02,\n",
       "          -1.7079e-02,  1.2405e-02,  5.3795e-02,  7.3061e-03, -5.1526e-03,\n",
       "          -2.5550e-02,  8.2727e-02, -2.6429e-02, -3.9453e-02, -1.9646e-03,\n",
       "          -6.4648e-03,  3.4287e-02,  3.8238e-02,  1.0599e-02, -6.2676e-02,\n",
       "          -5.9641e-02, -3.5666e-02,  6.7580e-03, -3.9476e-02, -1.3642e-02,\n",
       "          -1.1270e-02, -9.4461e-03, -3.9896e-02, -5.6277e-02, -3.7944e-02,\n",
       "           3.1640e-02,  5.6728e-03,  1.9673e-03, -1.0512e-02, -2.2282e-02,\n",
       "          -1.8795e-02, -2.7861e-04, -2.6127e-04, -7.2792e-04, -1.4703e-02,\n",
       "           1.1696e-02,  1.3888e-02,  2.4187e-02,  6.6908e-03, -2.1913e-02,\n",
       "          -1.2394e-02, -1.9968e-02, -2.6031e-02, -2.0840e-02, -9.2849e-03,\n",
       "           5.1386e-03, -1.4098e-03,  1.2841e-02, -4.3044e-02,  2.8219e-02,\n",
       "          -2.3291e-02, -4.0660e-02, -5.1213e-03, -8.4640e-03,  1.9698e-02,\n",
       "          -2.8856e-02, -3.8305e-02, -7.7872e-03, -4.3004e-02, -2.2549e-02,\n",
       "          -1.3849e-04, -3.7422e-02, -6.5485e-02, -2.0490e-02,  6.5027e-03,\n",
       "          -6.3166e-02,  6.1285e-02,  1.6469e-02,  4.1968e-02,  6.8143e-03,\n",
       "           5.3101e-02,  7.3946e-02, -1.4799e-02,  2.2720e-02,  1.4081e-02,\n",
       "           3.4664e-02,  3.5860e-02,  4.1299e-02,  2.0090e-03,  8.3383e-02,\n",
       "           5.8666e-03,  2.9274e-02,  5.4341e-02,  5.1473e-02,  5.8824e-03,\n",
       "          -1.1015e-01, -7.1227e-03,  2.1261e-03, -4.3606e-02,  1.4556e-02,\n",
       "           1.6076e-03,  6.4167e-02, -1.9888e-02,  1.7571e-02,  3.3746e-03,\n",
       "          -1.6678e-02,  1.5121e-02, -5.3681e-02,  3.6626e-02,  1.8840e-03,\n",
       "          -1.3665e-02,  2.2769e-03, -3.2425e-02, -3.4981e-02,  6.6593e-02,\n",
       "          -5.9788e-02,  1.0494e-02,  2.2928e-02,  2.7725e-02,  1.0261e-02,\n",
       "          -6.3776e-03,  1.9557e-02,  1.2576e-02, -3.7053e-02,  1.7500e-02,\n",
       "           3.0356e-02,  4.5247e-03,  9.4979e-05, -2.6920e-02,  6.4607e-02,\n",
       "          -2.2184e-02,  2.2535e-02,  1.4039e-02,  2.2537e-02,  4.0717e-02,\n",
       "           5.2517e-02,  1.3094e-02, -4.1000e-02, -1.4090e-02,  3.5917e-02,\n",
       "          -2.4394e-04,  1.6003e-02,  5.2418e-02,  7.9244e-03, -5.0230e-02,\n",
       "          -1.8618e-02, -3.3769e-03,  7.6580e-02,  5.0314e-03,  7.7542e-03,\n",
       "           5.6020e-02,  3.5139e-02,  5.8923e-02,  4.1169e-03, -5.8195e-03,\n",
       "          -2.3743e-03, -3.4920e-03, -3.6795e-02,  4.8496e-02,  2.0430e-02,\n",
       "           5.4108e-03,  6.5675e-02,  2.8157e-02,  3.3285e-02, -3.5258e-02,\n",
       "           1.6914e-02,  2.8489e-02, -9.5413e-02, -1.3485e-02, -1.6081e-02,\n",
       "           5.3349e-03,  4.2589e-02,  4.2560e-02, -3.1165e-02,  2.7384e-02,\n",
       "          -3.5466e-02,  9.3000e-03,  5.4778e-02, -2.0987e-02, -9.3251e-03,\n",
       "          -3.7665e-02,  2.2116e-02, -3.0540e-02,  3.8671e-02,  1.4434e-02,\n",
       "          -4.8408e-02, -4.0641e-02,  2.5366e-02,  1.9179e-03, -1.1063e-02,\n",
       "          -4.0137e-02,  1.5540e-02,  1.6394e-03, -2.3793e-02, -1.6851e-02,\n",
       "           5.3653e-03,  2.1577e-02, -2.6813e-03]]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.event_preprocessor.cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0216, -0.0204, -0.0031,  0.0263,  0.0095, -0.0296, -0.0817,\n",
       "          -0.0247,  0.0244, -0.0231,  0.0479, -0.0058,  0.0232,  0.0399,\n",
       "          -0.0922, -0.0184,  0.0244, -0.0361,  0.0175,  0.0034,  0.0123,\n",
       "           0.0208, -0.0182, -0.0053,  0.0406,  0.0445,  0.0111,  0.0472,\n",
       "          -0.0185,  0.0164, -0.0453, -0.0149, -0.0017, -0.0231,  0.0250,\n",
       "          -0.0359,  0.0543, -0.0092, -0.0140, -0.0057, -0.0096,  0.0006,\n",
       "           0.0372,  0.0211,  0.0089, -0.0156,  0.0044,  0.0141, -0.0540,\n",
       "          -0.0087,  0.0678, -0.0273, -0.0088,  0.0241,  0.0194,  0.0472,\n",
       "          -0.0228,  0.0368,  0.0102, -0.0241, -0.0740,  0.0107, -0.0082,\n",
       "           0.0195,  0.0530,  0.0257, -0.0187,  0.0066,  0.0205, -0.0340,\n",
       "           0.0405,  0.0110,  0.0159, -0.0156, -0.0035, -0.0348,  0.0279,\n",
       "           0.0346,  0.0485,  0.0110,  0.0020, -0.0161, -0.0078, -0.0630,\n",
       "          -0.0265, -0.0172,  0.0715,  0.0302, -0.0890,  0.0183, -0.0018,\n",
       "          -0.0184,  0.0354, -0.0040,  0.0428, -0.0537, -0.0088, -0.0792,\n",
       "          -0.0588,  0.0407,  0.0484, -0.0126, -0.0098, -0.0709,  0.0137,\n",
       "          -0.0461, -0.0070,  0.0370, -0.0661, -0.0028,  0.0735, -0.0052,\n",
       "           0.0060,  0.0723,  0.0341, -0.0393,  0.0339, -0.0541, -0.0174,\n",
       "           0.0149,  0.0136,  0.0076,  0.0458,  0.0030,  0.0803, -0.0073,\n",
       "           0.0131, -0.0387,  0.0314,  0.0228,  0.0173,  0.0349,  0.0607,\n",
       "           0.0036,  0.0408, -0.0065, -0.0224,  0.0106,  0.0076, -0.0254,\n",
       "          -0.0073, -0.0391, -0.0568, -0.0326,  0.0326,  0.0421, -0.0057,\n",
       "          -0.0561,  0.0494,  0.0489,  0.0012, -0.0162,  0.0350,  0.0443,\n",
       "          -0.0105,  0.0154, -0.0230,  0.0146,  0.0212, -0.0849,  0.0211,\n",
       "           0.0012,  0.0173,  0.0026, -0.0475,  0.0499, -0.0066, -0.0680,\n",
       "          -0.0134, -0.0311,  0.0710,  0.0495,  0.0049, -0.0815, -0.0289,\n",
       "          -0.0054, -0.0068, -0.0255, -0.0373,  0.0764,  0.0287, -0.0321,\n",
       "          -0.0440,  0.0221,  0.0345,  0.0108, -0.0087,  0.0302, -0.0387,\n",
       "           0.0167, -0.0211, -0.0080,  0.0100,  0.0078, -0.0049, -0.0047,\n",
       "           0.0028,  0.0503,  0.0204, -0.0518,  0.0159, -0.1036, -0.0004,\n",
       "           0.0236, -0.0441,  0.0030, -0.0286,  0.0059,  0.0356,  0.0013,\n",
       "           0.0227, -0.0366, -0.0267, -0.0574,  0.0014,  0.0291, -0.0112,\n",
       "          -0.0594, -0.0075,  0.0554,  0.0198, -0.0108, -0.0139,  0.0126,\n",
       "          -0.0072, -0.0291, -0.0274,  0.0077, -0.0447, -0.0111, -0.0359,\n",
       "          -0.0036, -0.0152,  0.0148, -0.0512, -0.0155,  0.0228,  0.0019,\n",
       "           0.0129,  0.0019, -0.0216,  0.0353, -0.0081,  0.0371,  0.0249,\n",
       "          -0.0149,  0.0174, -0.0269,  0.0081, -0.0139, -0.0268,  0.0235,\n",
       "          -0.0023, -0.0157, -0.0570,  0.0218,  0.0403,  0.0330, -0.0153,\n",
       "          -0.0293,  0.0153, -0.0139, -0.0057, -0.0087, -0.0160,  0.0362,\n",
       "          -0.0473, -0.0598,  0.0365, -0.0217, -0.0142,  0.0326,  0.0669,\n",
       "          -0.0050,  0.0230,  0.0325, -0.0252, -0.0134,  0.0046,  0.0201,\n",
       "          -0.0169,  0.0197,  0.0636, -0.0089, -0.0847,  0.0459, -0.0371,\n",
       "           0.0641,  0.0301,  0.0117,  0.0201,  0.0048, -0.0479, -0.0217,\n",
       "          -0.0750, -0.0448, -0.0043,  0.0265, -0.0724,  0.0320,  0.0766,\n",
       "           0.0338, -0.0277,  0.0268, -0.0153, -0.0222,  0.0131,  0.0026,\n",
       "          -0.0298,  0.0202, -0.0830,  0.0617,  0.0066,  0.0303, -0.0073,\n",
       "          -0.0845,  0.0241, -0.0384, -0.0331, -0.0127, -0.0056, -0.0371,\n",
       "           0.0338, -0.0173, -0.0134,  0.0670,  0.0348, -0.0359,  0.0049,\n",
       "          -0.0017, -0.0123,  0.0333,  0.0416,  0.0623,  0.0752,  0.0079,\n",
       "          -0.0331,  0.0253,  0.0209, -0.0246,  0.0207, -0.0024, -0.0158,\n",
       "          -0.0425,  0.0352,  0.0063,  0.0063,  0.0216,  0.0513, -0.0315,\n",
       "          -0.0201, -0.0349,  0.0188,  0.0164, -0.0304,  0.0103,  0.0269,\n",
       "          -0.0437, -0.0394, -0.0251, -0.0321,  0.0188, -0.0543, -0.0198,\n",
       "          -0.0030, -0.0185,  0.0525, -0.0692, -0.0600, -0.0697,  0.0099,\n",
       "          -0.0675,  0.0428, -0.0557,  0.0174,  0.0288,  0.0225,  0.0324,\n",
       "           0.0228, -0.0675, -0.0134, -0.0011,  0.0263,  0.0407, -0.0238,\n",
       "           0.0363, -0.0302,  0.0058, -0.0717,  0.0365, -0.0183, -0.0266,\n",
       "           0.0311,  0.0411,  0.0330,  0.0321, -0.0458, -0.0384,  0.0205,\n",
       "           0.0200, -0.0088, -0.0237,  0.0365, -0.0181, -0.0028, -0.0598,\n",
       "           0.0586,  0.0270, -0.0019,  0.0638,  0.0028, -0.0318, -0.0901,\n",
       "           0.0145,  0.0108, -0.0237,  0.0476, -0.0447,  0.0198, -0.0305,\n",
       "          -0.0008,  0.0192,  0.0614, -0.0022, -0.0481,  0.0148,  0.0212,\n",
       "          -0.0182, -0.0052, -0.0292,  0.0606, -0.0016,  0.0047, -0.0003,\n",
       "          -0.0400, -0.0053, -0.0450,  0.0041, -0.0197,  0.0479, -0.0496,\n",
       "           0.0200, -0.0136, -0.0379,  0.0462,  0.0124, -0.0162,  0.0604,\n",
       "          -0.0613,  0.0310,  0.0016,  0.0168,  0.0062,  0.0172,  0.0211,\n",
       "          -0.0567, -0.0814,  0.0466,  0.0378,  0.0713, -0.0252, -0.0141,\n",
       "          -0.0357,  0.0043, -0.0151, -0.0415, -0.0179,  0.0053, -0.0470,\n",
       "           0.0149,  0.0579, -0.0545, -0.0203,  0.0611, -0.0376, -0.0019,\n",
       "           0.0030, -0.0770,  0.0476,  0.0052,  0.0178, -0.0005,  0.0347,\n",
       "          -0.0397,  0.0154, -0.0376,  0.0382,  0.0285,  0.0189,  0.1074,\n",
       "          -0.0539,  0.0163, -0.0466,  0.0080,  0.0039, -0.0127, -0.0146,\n",
       "           0.0096, -0.0571, -0.0353,  0.0750, -0.0365, -0.0703,  0.0020,\n",
       "          -0.0467,  0.0050, -0.0441, -0.0450,  0.0301, -0.0036, -0.0073,\n",
       "          -0.0104,  0.0077,  0.0519,  0.0459,  0.0354,  0.0073, -0.0392,\n",
       "          -0.0407, -0.0046, -0.0672, -0.0534, -0.0444,  0.0259, -0.0071,\n",
       "          -0.0126,  0.0542,  0.0117,  0.0317, -0.0374,  0.0493, -0.0212,\n",
       "           0.0458, -0.0794, -0.0065,  0.0362,  0.0553, -0.0095, -0.0232,\n",
       "          -0.0080, -0.0097, -0.0084, -0.0431,  0.0530,  0.0213,  0.0053,\n",
       "          -0.0392,  0.0174,  0.0132,  0.0396, -0.0052,  0.0532,  0.0388,\n",
       "           0.0235,  0.0102,  0.0126,  0.0116,  0.0397,  0.0287,  0.0309,\n",
       "           0.0240, -0.0317, -0.0096,  0.0109, -0.0213,  0.0172, -0.0248,\n",
       "          -0.0500, -0.0069, -0.0383,  0.0277,  0.0124, -0.0232, -0.0595,\n",
       "          -0.1245, -0.0581, -0.0117,  0.0117,  0.0185, -0.0456, -0.0482,\n",
       "           0.0275,  0.0483, -0.0187,  0.0197,  0.0353, -0.0313, -0.0534,\n",
       "          -0.0274,  0.0174,  0.0497,  0.0624,  0.0379, -0.0034, -0.0100,\n",
       "           0.0221,  0.0044,  0.0292,  0.0026,  0.0805, -0.0619, -0.0349,\n",
       "           0.0266, -0.0054, -0.0083,  0.0077, -0.0254,  0.0090, -0.0450,\n",
       "           0.0541, -0.0344, -0.0304, -0.0292, -0.0371, -0.0120,  0.0287,\n",
       "           0.0184,  0.0300,  0.0151, -0.0206,  0.0110,  0.0348,  0.0227,\n",
       "           0.0213, -0.0231,  0.0162,  0.0652, -0.0004,  0.0378,  0.0269,\n",
       "           0.0609,  0.0117,  0.0460, -0.0273,  0.0306, -0.0276,  0.0010,\n",
       "          -0.0110, -0.0474, -0.0023, -0.0271, -0.0147, -0.0168,  0.0602,\n",
       "           0.0387, -0.0295, -0.0036, -0.0305, -0.0519,  0.0107, -0.0668,\n",
       "          -0.0101, -0.0045, -0.0148,  0.0209, -0.0017, -0.0540, -0.0008,\n",
       "           0.0122, -0.0343, -0.0366, -0.0088,  0.0030,  0.0170,  0.0234,\n",
       "           0.0417, -0.0217,  0.0333,  0.0537,  0.0203, -0.0032, -0.0244,\n",
       "           0.0479, -0.0261,  0.0594,  0.0006,  0.0247, -0.0123, -0.0625,\n",
       "           0.0243, -0.0412,  0.0176,  0.0005, -0.0308, -0.0240,  0.0395,\n",
       "           0.0100, -0.0341,  0.0447, -0.0206,  0.0059,  0.0009, -0.0480,\n",
       "           0.0250, -0.0622,  0.0385, -0.0323, -0.0434,  0.0359,  0.0323,\n",
       "           0.0315,  0.0165,  0.0497, -0.0281, -0.0100, -0.0090, -0.0564,\n",
       "          -0.0004,  0.0172,  0.0476, -0.0067,  0.0204,  0.0184, -0.0093,\n",
       "          -0.0123,  0.0007,  0.0437, -0.0016,  0.0035,  0.0110,  0.0039,\n",
       "           0.0046, -0.0447,  0.0363,  0.0304,  0.0286, -0.0419,  0.0121,\n",
       "           0.1001, -0.0045, -0.0011, -0.0196,  0.0419,  0.0384,  0.0223,\n",
       "          -0.0189, -0.0012,  0.0038, -0.0227,  0.0389, -0.0476,  0.0486,\n",
       "           0.0058,  0.0594, -0.0107, -0.0770, -0.0432,  0.0865,  0.0289,\n",
       "          -0.0150,  0.0171, -0.0404,  0.0697, -0.0197, -0.0316,  0.0274,\n",
       "          -0.0428, -0.0058,  0.0311, -0.0026, -0.0062, -0.0109,  0.0072,\n",
       "          -0.0080, -0.0248, -0.0273, -0.0233,  0.0119]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.event_preprocessor.state_dict()['cls_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.load_weights(path='.checkpoints/imagebind_huge.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
