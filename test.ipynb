{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import torch\n",
    "from models import imagebind_model\n",
    "from models.imagebind_model import ModalityType\n",
    "\n",
    "text_list=[\"A dog.\", \"A car\", \"A bird\"]\n",
    "image_paths=[\".assets/dog_image.jpg\", \".assets/car_image.jpg\", \".assets/bird_image.jpg\"]\n",
    "audio_paths=[\".assets/dog_audio.wav\", \".assets/car_audio.wav\", \".assets/bird_audio.wav\"]\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Load data\n",
    "inputs = {\n",
    "    ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "    ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n",
    "    ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(inputs)\n",
    "\n",
    "print(\n",
    "    \"Vision x Text: \",\n",
    "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    ")\n",
    "print(\n",
    "    \"Audio x Text: \",\n",
    "    torch.softmax(embeddings[ModalityType.AUDIO] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    ")\n",
    "print(\n",
    "    \"Vision x Audio: \",\n",
    "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.AUDIO].T, dim=-1),\n",
    ")\n",
    "\n",
    "# Expected output:\n",
    "#\n",
    "# Vision x Text:\n",
    "# tensor([[9.9761e-01, 2.3694e-03, 1.8612e-05],\n",
    "#         [3.3836e-05, 9.9994e-01, 2.4118e-05],\n",
    "#         [4.7997e-05, 1.3496e-02, 9.8646e-01]])\n",
    "#\n",
    "# Audio x Text:\n",
    "# tensor([[1., 0., 0.],\n",
    "#         [0., 1., 0.],\n",
    "#         [0., 0., 1.]])\n",
    "#\n",
    "# Vision x Audio:\n",
    "# tensor([[0.8070, 0.1088, 0.0842],\n",
    "#         [0.1036, 0.7884, 0.1079],\n",
    "#         [0.0018, 0.0022, 0.9960]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.AUDIO].T, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.multimodal_preprocessors import PatchEmbedGeneric,PadIm2Video\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "class PatchEmbedGeneric(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchEmbed from Hydra\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, proj_stem, norm_layer: Optional[nn.Module] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        if len(proj_stem) > 1:\n",
    "            self.proj = nn.Sequential(*proj_stem)\n",
    "        else:\n",
    "            # Special case to be able to load pre-trained models that were\n",
    "            # trained with a standard stem\n",
    "            self.proj = proj_stem[0]\n",
    "        self.norm_layer = norm_layer\n",
    "\n",
    "    def get_patch_layout(self, img_size):\n",
    "        with torch.no_grad():\n",
    "            dummy_img = torch.zeros(\n",
    "                [\n",
    "                    1,\n",
    "                ]\n",
    "                + img_size\n",
    "            )\n",
    "            print(dummy_img.shape)\n",
    "            dummy_out = self.proj(dummy_img)\n",
    "        print(dummy_out.shape)\n",
    "        embed_dim = dummy_out.shape[1]\n",
    "        patches_layout = tuple(dummy_out.shape[2:])\n",
    "        num_patches = np.prod(patches_layout)\n",
    "        return patches_layout, num_patches, embed_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = self.proj(x)\n",
    "        print(x.shape)\n",
    "        # B C (T) H W -> B (T)HW C\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        print(x.shape)\n",
    "        if self.norm_layer is not None:\n",
    "            x = self.norm_layer(x)\n",
    "        return x\n",
    "    \n",
    "kernel_size=(2, 14, 14)\n",
    "vision_embed_dim=1024\n",
    "proj_stem=[\n",
    "                PadIm2Video(pad_type=\"repeat\", ntimes=2),\n",
    "                nn.Conv3d(\n",
    "                    in_channels=3,\n",
    "                    kernel_size=kernel_size,\n",
    "                    out_channels=vision_embed_dim,\n",
    "                    stride=kernel_size,\n",
    "                    bias=False,\n",
    "                )\n",
    "]\n",
    "PatchEmbedGeneric(proj_stem,None).get_patch_layout([3, 2,224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.helpers import VerboseNNModule\n",
    "from typing import Tuple, Optional, Callable\n",
    "from models.helpers import (EinOpsRearrange, LearnableLogitScaling, Normalize,\n",
    "                            SelectElement, SelectEOSAndProject)\n",
    "from models.multimodal_preprocessors import (AudioPreprocessor,\n",
    "                                             IMUPreprocessor, PadIm2Video,\n",
    "                                             PatchEmbedGeneric,\n",
    "                                             RGBDTPreprocessor,\n",
    "                                             SpatioTemporalPosEmbeddingHelper,\n",
    "                                             TextPreprocessor,\n",
    "                                             ThermalPreprocessor)\n",
    "from models.transformer import MultiheadAttention, SimpleTransformer\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from functools import partial\n",
    "from types import SimpleNamespace\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RGBDTPreprocessor(VerboseNNModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rgbt_stem: PatchEmbedGeneric,\n",
    "        depth_stem: Optional[PatchEmbedGeneric],\n",
    "        img_size: Tuple = (3, 224, 224),\n",
    "        num_cls_tokens: int = 1,\n",
    "        pos_embed_fn: Optional[Callable] = None,\n",
    "        use_type_embed: bool = False,\n",
    "        init_param_style: str = \"openclip\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        stem = rgbt_stem if rgbt_stem is not None else depth_stem\n",
    "        (\n",
    "            self.patches_layout,\n",
    "            self.num_patches,\n",
    "            self.embed_dim,\n",
    "        ) = stem.get_patch_layout(img_size)\n",
    "        self.rgbt_stem = rgbt_stem\n",
    "        self.depth_stem = depth_stem\n",
    "        self.use_pos_embed = pos_embed_fn is not None\n",
    "        self.use_type_embed = use_type_embed\n",
    "        self.num_cls_tokens = num_cls_tokens\n",
    "\n",
    "        if self.use_pos_embed:\n",
    "            self.pos_embedding_helper = pos_embed_fn(\n",
    "                patches_layout=self.patches_layout,\n",
    "                num_cls_tokens=num_cls_tokens,\n",
    "                num_patches=self.num_patches,\n",
    "                embed_dim=self.embed_dim,\n",
    "            )\n",
    "        if self.num_cls_tokens > 0:\n",
    "            self.cls_token = nn.Parameter(\n",
    "                torch.zeros(1, self.num_cls_tokens, self.embed_dim)\n",
    "            )\n",
    "        if self.use_type_embed:\n",
    "            self.type_embed = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "\n",
    "        self.init_parameters(init_param_style)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_parameters(self, init_param_style):\n",
    "        if init_param_style == \"openclip\":\n",
    "            # OpenCLIP style initialization\n",
    "            scale = self.embed_dim**-0.5\n",
    "            if self.use_pos_embed:\n",
    "                nn.init.normal_(self.pos_embedding_helper.pos_embed)\n",
    "                self.pos_embedding_helper.pos_embed *= scale\n",
    "\n",
    "            if self.num_cls_tokens > 0:\n",
    "                nn.init.normal_(self.cls_token)\n",
    "                self.cls_token *= scale\n",
    "        elif init_param_style == \"vit\":\n",
    "            self.cls_token.data.fill_(0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown init {init_param_style}\")\n",
    "\n",
    "        if self.use_type_embed:\n",
    "            nn.init.normal_(self.type_embed)\n",
    "\n",
    "    def tokenize_input_and_cls_pos(self, input, stem, mask):\n",
    "        # tokens is of shape B x L x D\n",
    "        tokens = stem(input)\n",
    "        assert tokens.ndim == 3\n",
    "        assert tokens.shape[2] == self.embed_dim\n",
    "        B = tokens.shape[0]\n",
    "        if self.num_cls_tokens > 0:\n",
    "            class_tokens = self.cls_token.expand(\n",
    "                B, -1, -1\n",
    "            )  # stole class_tokens impl from Phil Wang, thanks\n",
    "            tokens = torch.cat((class_tokens, tokens), dim=1)\n",
    "        if self.use_pos_embed:\n",
    "            pos_embed = self.pos_embedding_helper.get_pos_embedding(input, tokens)\n",
    "            tokens = tokens + pos_embed\n",
    "        if self.use_type_embed:\n",
    "            tokens = tokens + self.type_embed.expand(B, -1, -1)\n",
    "        return tokens\n",
    "\n",
    "    def forward(self, vision=None, depth=None, patch_mask=None):\n",
    "        if patch_mask is not None:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        if vision is not None:\n",
    "            vision_tokens = self.tokenize_input_and_cls_pos(\n",
    "                vision, self.rgbt_stem, patch_mask\n",
    "            )\n",
    "\n",
    "        if depth is not None:\n",
    "            depth_tokens = self.tokenize_input_and_cls_pos(\n",
    "                depth, self.depth_stem, patch_mask\n",
    "            )\n",
    "\n",
    "        # aggregate tokens\n",
    "        if vision is not None and depth is not None:\n",
    "            final_tokens = vision_tokens + depth_tokens\n",
    "        else:\n",
    "            final_tokens = vision_tokens if vision is not None else depth_tokens\n",
    "        return_dict = {\n",
    "            \"trunk\": {\n",
    "                \"tokens\": final_tokens,\n",
    "            },\n",
    "            \"head\": {},\n",
    "        }\n",
    "        return return_dict\n",
    "\n",
    "rgbt_stem = PatchEmbedGeneric(\n",
    "            proj_stem=[\n",
    "                PadIm2Video(pad_type=\"repeat\", ntimes=2),\n",
    "                nn.Conv3d(\n",
    "                    in_channels=3,\n",
    "                    kernel_size=kernel_size,\n",
    "                    out_channels=vision_embed_dim,\n",
    "                    stride=kernel_size,\n",
    "                    bias=False,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "rgbt_preprocessor = RGBDTPreprocessor(\n",
    "            img_size=[3, 2, 224, 224],\n",
    "            num_cls_tokens=1,\n",
    "            pos_embed_fn=partial(SpatioTemporalPosEmbeddingHelper, learnable=True),\n",
    "            rgbt_stem=rgbt_stem,\n",
    "            depth_stem=None,\n",
    "        )\n",
    "\n",
    "rgbt_preprocessor(dummy_img[0])['trunk']['tokens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.imagebind_model import ImageBindModel\n",
    "imageBind=ImageBindModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dummy_img = torch.zeros(\n",
    "                [\n",
    "                    1,\n",
    "                ]\n",
    "                + [3,2, 224, 224]\n",
    "            )\n",
    "imageBind.layer_shapes(dummy_img,modality_type=\"vision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.multimodal_preprocessors import PatchEmbedGeneric,PadIm2Video\n",
    "from torchvision import transforms\n",
    "from datasets.VideoDataset import resize_pad,VideoDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "dummy_img = torch.zeros(\n",
    "                [3, 500, 500]\n",
    "            )\n",
    "# PadIm2Video(pad_type=\"repeat\", ntimes=2)(dummy_img).shape\n",
    "videodataset=VideoDataset()\n",
    "data=videodataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose to B, T, C, H, W\n",
    "data=data[0].permute(0,2,1,3,4)\n",
    "# visualize first frame\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(data[0,0].permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "path='.checkpoints/imagebind_huge.pth'\n",
    "state_dict = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict['modality_heads.thermal.2.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[keys for keys in state_dict.keys() if 'thermal' in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[key for key in state_dict.keys() if key.startswith(\"modality_preprocessors.thermal\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.events import EventModel\n",
    "e=EventModel()\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.event_preprocessor.cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.event_preprocessor.state_dict()['cls_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.load_weights(path='.checkpoints/imagebind_huge.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/tsukimi/datasets/Chiba/finetune_train/HL-HC-Official-2023_12_04_14_10_37-100.pkl'\n",
    "import pickle as pkl\n",
    "with open(path, 'rb') as f:\n",
    "    data = pkl.load(f)\n",
    "from datasets.EventDataset import events_to_image as e2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events=data['events'][0]\n",
    "events['polarity'][events['polarity']==0]=-1\n",
    "events_positive=events[events['polarity']==1]\n",
    "events_negative=events[events['polarity']==-1]\n",
    "event_frame_positive=e2i(events_positive['x'],events_positive['y'],events_positive['polarity'])\n",
    "event_frame_negative=e2i(events_negative['x'],events_negative['y'],events_negative['polarity'])\n",
    "#find the indexes where eventframe not equals to 0\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(event_frame_negative, cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(event_frame_positive, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat image\n",
    "import numpy as np\n",
    "event_frame=np.stack([event_frame_positive,event_frame_negative,event_frame_positive+event_frame_negative])\n",
    "plt.imshow(np.transpose(event_frame,(1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.EventDataset import events_to_image_torch as e2it\n",
    "import torch\n",
    "events=data['events'][0]\n",
    "events['polarity'][events['polarity']==0]=-1\n",
    "events_positive=events[events['polarity']==1]\n",
    "events_negative=events[events['polarity']==-1]\n",
    "\n",
    "# deep compare e2it(events_negative['x'],events_negative['y'],events_negative['polarity']) and event_frame_negative\n",
    "events_positive_frame=e2it(events_positive['x'],events_positive['y'],events_positive['polarity'])\n",
    "events_negative_frame=e2it(events_negative['x'],events_negative['y'],events_negative['polarity'])\n",
    "event_frame=torch.stack([events_positive_frame,events_negative_frame,events_positive_frame+events_negative_frame],dim=0)\n",
    "event_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_path={'train':['/tsukimi/datasets/Chiba/finetune_train/HL-HC-Official-2023_12_04_14_10_37-100.pkl']}\n",
    "import pickle\n",
    "#save dummy_path\n",
    "with open('dummy_path.pkl', 'wb') as f:\n",
    "    pickle.dump(dummy_path, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.EventDataset import EventDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "eventdataset=EventDataset(mode='train',data_dir='/tsukimi/datasets/Chiba/finetune_train/',path='dummy_path.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventdataset[0]['image_units'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path='/tsukimi/datasets/Chiba/baseline/datalist_3'\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "sum = torch.tensor([0.0, 0.0, 0.0])\n",
    "sum_of_squares = torch.tensor([0.0, 0.0, 0.0])\n",
    "num_pixels = 0\n",
    "\n",
    "\n",
    "for name in ['train','val','test']:\n",
    "    data=pd.read_csv(csv_path+f'/{name}.csv')\n",
    "    videos=list(data.values[:,0])\n",
    "    for video in videos:\n",
    "        # Open video file\n",
    "        cap = cv2.VideoCapture(video)\n",
    "        \n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            \n",
    "            # Convert frame to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Convert to tensor and normalize to [0, 1]\n",
    "            frame = torch.tensor(frame).permute(2, 0, 1).float() / 255.0\n",
    "            \n",
    "            # Calculate sum and sum of squares\n",
    "            sum += frame.sum([1, 2])\n",
    "            sum_of_squares += (frame ** 2).sum([1, 2])\n",
    "            num_pixels += frame.size(1) * frame.size(2)\n",
    "        \n",
    "        cap.release()\n",
    "\n",
    "# Calculate mean and std\n",
    "mean = sum / num_pixels\n",
    "std = (sum_of_squares / num_pixels - mean ** 2) ** 0.5\n",
    "print('Mean:', mean)\n",
    "print('Std:', std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.VideoDataset import VideoDataset\n",
    "from models import imagebind_model\n",
    "import torch\n",
    "imagebind=imagebind_model.imagebind_huge(pretrained=True)\n",
    "imagebind.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.zeros(\n",
    "                [\n",
    "                    3,2\n",
    "                ]\n",
    "                + [3, 2,224, 224]\n",
    "            )\n",
    "with torch.no_grad():\n",
    "    embeddings = imagebind({\"vision\":dummy_input})\n",
    "embeddings['vision'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/imagebind/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from train_baseline import VideoTrain\n",
    "from datasets.VideoDataset import VideoDataModule\n",
    "\n",
    "train=VideoTrain()\n",
    "# load batch\n",
    "dataset=VideoDataModule(csv_path='/tsukimi/datasets/Chiba/baseline/datalist_3')\n",
    "loader=dataset.train_dataloader()\n",
    "batch,labels=next(iter(loader))\n",
    "embed=train(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0780,  0.0870, -0.0244]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/imagebind/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9856\n"
     ]
    }
   ],
   "source": [
    "from datasets.VideoDataset import VideoDataModule\n",
    "module=VideoDataModule(csv_path='/tsukimi/datasets/Chiba/baseline/datalist_3')\n",
    "module.setup(stage='fit')\n",
    "loader=module.train_dataloader()\n",
    "print(len(loader))\n",
    "batch=next(iter(loader))\n",
    "from train_baseline import VideoTrain\n",
    "train=VideoTrain()\n",
    "embed=train(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0755, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from eventutils import AccMetric,ConfusionMatrixMetric, multi_label_accuracy, custom_multi_label_pred, ground_truth_decoder\n",
    "from pytorch_loss import FocalLossV3\n",
    "labels=batch[1]\n",
    "gt=ground_truth_decoder(labels)\n",
    "c=FocalLossV3()\n",
    "c(embed.to('cuda'),gt.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 42, 3, 2, 224, 224])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "next(iter(loader))[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
