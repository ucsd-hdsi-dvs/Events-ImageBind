{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import torch\n",
    "from models import imagebind_model\n",
    "from models.imagebind_model import ModalityType\n",
    "\n",
    "text_list=[\"A dog.\", \"A car\", \"A bird\"]\n",
    "image_paths=[\".assets/dog_image.jpg\", \".assets/car_image.jpg\", \".assets/bird_image.jpg\"]\n",
    "audio_paths=[\".assets/dog_audio.wav\", \".assets/car_audio.wav\", \".assets/bird_audio.wav\"]\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Load data\n",
    "inputs = {\n",
    "    ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "    ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n",
    "    ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(inputs)\n",
    "\n",
    "print(\n",
    "    \"Vision x Text: \",\n",
    "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    ")\n",
    "print(\n",
    "    \"Audio x Text: \",\n",
    "    torch.softmax(embeddings[ModalityType.AUDIO] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    ")\n",
    "print(\n",
    "    \"Vision x Audio: \",\n",
    "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.AUDIO].T, dim=-1),\n",
    ")\n",
    "\n",
    "# Expected output:\n",
    "#\n",
    "# Vision x Text:\n",
    "# tensor([[9.9761e-01, 2.3694e-03, 1.8612e-05],\n",
    "#         [3.3836e-05, 9.9994e-01, 2.4118e-05],\n",
    "#         [4.7997e-05, 1.3496e-02, 9.8646e-01]])\n",
    "#\n",
    "# Audio x Text:\n",
    "# tensor([[1., 0., 0.],\n",
    "#         [0., 1., 0.],\n",
    "#         [0., 0., 1.]])\n",
    "#\n",
    "# Vision x Audio:\n",
    "# tensor([[0.8070, 0.1088, 0.0842],\n",
    "#         [0.1036, 0.7884, 0.1079],\n",
    "#         [0.0018, 0.0022, 0.9960]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.AUDIO].T, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.multimodal_preprocessors import PatchEmbedGeneric,PadIm2Video\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "class PatchEmbedGeneric(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchEmbed from Hydra\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, proj_stem, norm_layer: Optional[nn.Module] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        if len(proj_stem) > 1:\n",
    "            self.proj = nn.Sequential(*proj_stem)\n",
    "        else:\n",
    "            # Special case to be able to load pre-trained models that were\n",
    "            # trained with a standard stem\n",
    "            self.proj = proj_stem[0]\n",
    "        self.norm_layer = norm_layer\n",
    "\n",
    "    def get_patch_layout(self, img_size):\n",
    "        with torch.no_grad():\n",
    "            dummy_img = torch.zeros(\n",
    "                [\n",
    "                    1,\n",
    "                ]\n",
    "                + img_size\n",
    "            )\n",
    "            print(dummy_img.shape)\n",
    "            dummy_out = self.proj(dummy_img)\n",
    "        print(dummy_out.shape)\n",
    "        embed_dim = dummy_out.shape[1]\n",
    "        patches_layout = tuple(dummy_out.shape[2:])\n",
    "        num_patches = np.prod(patches_layout)\n",
    "        return patches_layout, num_patches, embed_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = self.proj(x)\n",
    "        print(x.shape)\n",
    "        # B C (T) H W -> B (T)HW C\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        print(x.shape)\n",
    "        if self.norm_layer is not None:\n",
    "            x = self.norm_layer(x)\n",
    "        return x\n",
    "    \n",
    "kernel_size=(2, 14, 14)\n",
    "vision_embed_dim=1024\n",
    "proj_stem=[\n",
    "                PadIm2Video(pad_type=\"repeat\", ntimes=2),\n",
    "                nn.Conv3d(\n",
    "                    in_channels=3,\n",
    "                    kernel_size=kernel_size,\n",
    "                    out_channels=vision_embed_dim,\n",
    "                    stride=kernel_size,\n",
    "                    bias=False,\n",
    "                )\n",
    "]\n",
    "PatchEmbedGeneric(proj_stem,None).get_patch_layout([3, 2,224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.helpers import VerboseNNModule\n",
    "from typing import Tuple, Optional, Callable\n",
    "from models.helpers import (EinOpsRearrange, LearnableLogitScaling, Normalize,\n",
    "                            SelectElement, SelectEOSAndProject)\n",
    "from models.multimodal_preprocessors import (AudioPreprocessor,\n",
    "                                             IMUPreprocessor, PadIm2Video,\n",
    "                                             PatchEmbedGeneric,\n",
    "                                             RGBDTPreprocessor,\n",
    "                                             SpatioTemporalPosEmbeddingHelper,\n",
    "                                             TextPreprocessor,\n",
    "                                             ThermalPreprocessor)\n",
    "from models.transformer import MultiheadAttention, SimpleTransformer\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from functools import partial\n",
    "from types import SimpleNamespace\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RGBDTPreprocessor(VerboseNNModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rgbt_stem: PatchEmbedGeneric,\n",
    "        depth_stem: Optional[PatchEmbedGeneric],\n",
    "        img_size: Tuple = (3, 224, 224),\n",
    "        num_cls_tokens: int = 1,\n",
    "        pos_embed_fn: Optional[Callable] = None,\n",
    "        use_type_embed: bool = False,\n",
    "        init_param_style: str = \"openclip\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        stem = rgbt_stem if rgbt_stem is not None else depth_stem\n",
    "        (\n",
    "            self.patches_layout,\n",
    "            self.num_patches,\n",
    "            self.embed_dim,\n",
    "        ) = stem.get_patch_layout(img_size)\n",
    "        self.rgbt_stem = rgbt_stem\n",
    "        self.depth_stem = depth_stem\n",
    "        self.use_pos_embed = pos_embed_fn is not None\n",
    "        self.use_type_embed = use_type_embed\n",
    "        self.num_cls_tokens = num_cls_tokens\n",
    "\n",
    "        if self.use_pos_embed:\n",
    "            self.pos_embedding_helper = pos_embed_fn(\n",
    "                patches_layout=self.patches_layout,\n",
    "                num_cls_tokens=num_cls_tokens,\n",
    "                num_patches=self.num_patches,\n",
    "                embed_dim=self.embed_dim,\n",
    "            )\n",
    "        if self.num_cls_tokens > 0:\n",
    "            self.cls_token = nn.Parameter(\n",
    "                torch.zeros(1, self.num_cls_tokens, self.embed_dim)\n",
    "            )\n",
    "        if self.use_type_embed:\n",
    "            self.type_embed = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "\n",
    "        self.init_parameters(init_param_style)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_parameters(self, init_param_style):\n",
    "        if init_param_style == \"openclip\":\n",
    "            # OpenCLIP style initialization\n",
    "            scale = self.embed_dim**-0.5\n",
    "            if self.use_pos_embed:\n",
    "                nn.init.normal_(self.pos_embedding_helper.pos_embed)\n",
    "                self.pos_embedding_helper.pos_embed *= scale\n",
    "\n",
    "            if self.num_cls_tokens > 0:\n",
    "                nn.init.normal_(self.cls_token)\n",
    "                self.cls_token *= scale\n",
    "        elif init_param_style == \"vit\":\n",
    "            self.cls_token.data.fill_(0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown init {init_param_style}\")\n",
    "\n",
    "        if self.use_type_embed:\n",
    "            nn.init.normal_(self.type_embed)\n",
    "\n",
    "    def tokenize_input_and_cls_pos(self, input, stem, mask):\n",
    "        # tokens is of shape B x L x D\n",
    "        tokens = stem(input)\n",
    "        assert tokens.ndim == 3\n",
    "        assert tokens.shape[2] == self.embed_dim\n",
    "        B = tokens.shape[0]\n",
    "        if self.num_cls_tokens > 0:\n",
    "            class_tokens = self.cls_token.expand(\n",
    "                B, -1, -1\n",
    "            )  # stole class_tokens impl from Phil Wang, thanks\n",
    "            tokens = torch.cat((class_tokens, tokens), dim=1)\n",
    "        if self.use_pos_embed:\n",
    "            pos_embed = self.pos_embedding_helper.get_pos_embedding(input, tokens)\n",
    "            tokens = tokens + pos_embed\n",
    "        if self.use_type_embed:\n",
    "            tokens = tokens + self.type_embed.expand(B, -1, -1)\n",
    "        return tokens\n",
    "\n",
    "    def forward(self, vision=None, depth=None, patch_mask=None):\n",
    "        if patch_mask is not None:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        if vision is not None:\n",
    "            vision_tokens = self.tokenize_input_and_cls_pos(\n",
    "                vision, self.rgbt_stem, patch_mask\n",
    "            )\n",
    "\n",
    "        if depth is not None:\n",
    "            depth_tokens = self.tokenize_input_and_cls_pos(\n",
    "                depth, self.depth_stem, patch_mask\n",
    "            )\n",
    "\n",
    "        # aggregate tokens\n",
    "        if vision is not None and depth is not None:\n",
    "            final_tokens = vision_tokens + depth_tokens\n",
    "        else:\n",
    "            final_tokens = vision_tokens if vision is not None else depth_tokens\n",
    "        return_dict = {\n",
    "            \"trunk\": {\n",
    "                \"tokens\": final_tokens,\n",
    "            },\n",
    "            \"head\": {},\n",
    "        }\n",
    "        return return_dict\n",
    "\n",
    "rgbt_stem = PatchEmbedGeneric(\n",
    "            proj_stem=[\n",
    "                PadIm2Video(pad_type=\"repeat\", ntimes=2),\n",
    "                nn.Conv3d(\n",
    "                    in_channels=3,\n",
    "                    kernel_size=kernel_size,\n",
    "                    out_channels=vision_embed_dim,\n",
    "                    stride=kernel_size,\n",
    "                    bias=False,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "rgbt_preprocessor = RGBDTPreprocessor(\n",
    "            img_size=[3, 2, 224, 224],\n",
    "            num_cls_tokens=1,\n",
    "            pos_embed_fn=partial(SpatioTemporalPosEmbeddingHelper, learnable=True),\n",
    "            rgbt_stem=rgbt_stem,\n",
    "            depth_stem=None,\n",
    "        )\n",
    "\n",
    "rgbt_preprocessor(dummy_img[0])['trunk']['tokens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.imagebind_model import ImageBindModel\n",
    "imageBind=ImageBindModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dummy_img = torch.zeros(\n",
    "                [\n",
    "                    1,\n",
    "                ]\n",
    "                + [3,2, 224, 224]\n",
    "            )\n",
    "imageBind.layer_shapes(dummy_img,modality_type=\"vision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.multimodal_preprocessors import PatchEmbedGeneric,PadIm2Video\n",
    "from torchvision import transforms\n",
    "from datasets.VideoDataset import resize_pad,VideoDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "dummy_img = torch.zeros(\n",
    "                [3, 500, 500]\n",
    "            )\n",
    "# PadIm2Video(pad_type=\"repeat\", ntimes=2)(dummy_img).shape\n",
    "videodataset=VideoDataset()\n",
    "data=videodataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose to B, T, C, H, W\n",
    "data=data[0].permute(0,2,1,3,4)\n",
    "# visualize first frame\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(data[0,0].permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "path='.checkpoints/imagebind_huge.pth'\n",
    "state_dict = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict['modality_heads.thermal.2.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[keys for keys in state_dict.keys() if 'thermal' in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[key for key in state_dict.keys() if key.startswith(\"modality_preprocessors.thermal\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.events import EventModel\n",
    "e=EventModel()\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.event_preprocessor.cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.event_preprocessor.state_dict()['cls_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.load_weights(path='.checkpoints/imagebind_huge.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/tsukimi/datasets/Chiba/finetune_train/HL-HC-Official-2023_12_04_14_10_37-100.pkl'\n",
    "import pickle as pkl\n",
    "with open(path, 'rb') as f:\n",
    "    data = pkl.load(f)\n",
    "from datasets.EventDataset import events_to_image as e2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events=data['events'][0]\n",
    "events['polarity'][events['polarity']==0]=-1\n",
    "events_positive=events[events['polarity']==1]\n",
    "events_negative=events[events['polarity']==-1]\n",
    "event_frame_positive=e2i(events_positive['x'],events_positive['y'],events_positive['polarity'])\n",
    "event_frame_negative=e2i(events_negative['x'],events_negative['y'],events_negative['polarity'])\n",
    "#find the indexes where eventframe not equals to 0\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(event_frame_negative, cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(event_frame_positive, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat image\n",
    "import numpy as np\n",
    "event_frame=np.stack([event_frame_positive,event_frame_negative,event_frame_positive+event_frame_negative])\n",
    "plt.imshow(np.transpose(event_frame,(1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.EventDataset import events_to_image_torch as e2it\n",
    "import torch\n",
    "events=data['events'][0]\n",
    "events['polarity'][events['polarity']==0]=-1\n",
    "events_positive=events[events['polarity']==1]\n",
    "events_negative=events[events['polarity']==-1]\n",
    "\n",
    "# deep compare e2it(events_negative['x'],events_negative['y'],events_negative['polarity']) and event_frame_negative\n",
    "events_positive_frame=e2it(events_positive['x'],events_positive['y'],events_positive['polarity'])\n",
    "events_negative_frame=e2it(events_negative['x'],events_negative['y'],events_negative['polarity'])\n",
    "event_frame=torch.stack([events_positive_frame,events_negative_frame,events_positive_frame+events_negative_frame],dim=0)\n",
    "event_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_path={'train':['/tsukimi/datasets/Chiba/finetune_train/HL-HC-Official-2023_12_04_14_10_37-100.pkl']}\n",
    "import pickle\n",
    "#save dummy_path\n",
    "with open('dummy_path.pkl', 'wb') as f:\n",
    "    pickle.dump(dummy_path, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.EventDataset import EventDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "eventdataset=EventDataset(mode='train',data_dir='/tsukimi/datasets/Chiba/finetune_train/',path='dummy_path.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventdataset[0]['image_units'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path='/tsukimi/datasets/Chiba/baseline/datalist_3'\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "sum = torch.tensor([0.0, 0.0, 0.0])\n",
    "sum_of_squares = torch.tensor([0.0, 0.0, 0.0])\n",
    "num_pixels = 0\n",
    "\n",
    "\n",
    "for name in ['train','val','test']:\n",
    "    data=pd.read_csv(csv_path+f'/{name}.csv')\n",
    "    videos=list(data.values[:,0])\n",
    "    for video in videos:\n",
    "        # Open video file\n",
    "        cap = cv2.VideoCapture(video)\n",
    "        \n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            \n",
    "            # Convert frame to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Convert to tensor and normalize to [0, 1]\n",
    "            frame = torch.tensor(frame).permute(2, 0, 1).float() / 255.0\n",
    "            \n",
    "            # Calculate sum and sum of squares\n",
    "            sum += frame.sum([1, 2])\n",
    "            sum_of_squares += (frame ** 2).sum([1, 2])\n",
    "            num_pixels += frame.size(1) * frame.size(2)\n",
    "        \n",
    "        cap.release()\n",
    "\n",
    "# Calculate mean and std\n",
    "mean = sum / num_pixels\n",
    "std = (sum_of_squares / num_pixels - mean ** 2) ** 0.5\n",
    "print('Mean:', mean)\n",
    "print('Std:', std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/imagebind/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImageBindModel(\n",
       "  (modality_preprocessors): ModuleDict(\n",
       "    (vision): RGBDTPreprocessor(\n",
       "      (cls_token): tensor((1, 1, 1280), requires_grad=True)\n",
       "      \n",
       "      (rgbt_stem): PatchEmbedGeneric(\n",
       "        (proj): Sequential(\n",
       "          (0): PadIm2Video()\n",
       "          (1): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
       "        (pos_embed): tensor((1, 257, 1280), requires_grad=True)\n",
       "        \n",
       "      )\n",
       "    )\n",
       "    (text): TextPreprocessor(\n",
       "      (pos_embed): tensor((1, 77, 1024), requires_grad=True)\n",
       "      (mask): tensor((77, 77), requires_grad=False)\n",
       "      \n",
       "      (token_embedding): Embedding(49408, 1024)\n",
       "    )\n",
       "    (audio): AudioPreprocessor(\n",
       "      (cls_token): tensor((1, 1, 768), requires_grad=True)\n",
       "      \n",
       "      (rgbt_stem): PatchEmbedGeneric(\n",
       "        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10), bias=False)\n",
       "        (norm_layer): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
       "        (pos_embed): tensor((1, 229, 768), requires_grad=True)\n",
       "        \n",
       "      )\n",
       "    )\n",
       "    (depth): RGBDTPreprocessor(\n",
       "      (cls_token): tensor((1, 1, 384), requires_grad=True)\n",
       "      \n",
       "      (depth_stem): PatchEmbedGeneric(\n",
       "        (proj): Conv2d(1, 384, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "        (norm_layer): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
       "        (pos_embed): tensor((1, 197, 384), requires_grad=True)\n",
       "        \n",
       "      )\n",
       "    )\n",
       "    (thermal): ThermalPreprocessor(\n",
       "      (cls_token): tensor((1, 1, 768), requires_grad=True)\n",
       "      \n",
       "      (rgbt_stem): PatchEmbedGeneric(\n",
       "        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "        (norm_layer): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
       "        (pos_embed): tensor((1, 197, 768), requires_grad=True)\n",
       "        \n",
       "      )\n",
       "    )\n",
       "    (imu): IMUPreprocessor(\n",
       "      (pos_embed): tensor((1, 251, 512), requires_grad=True)\n",
       "      (cls_token): tensor((1, 1, 512), requires_grad=True)\n",
       "      \n",
       "      (imu_stem): PatchEmbedGeneric(\n",
       "        (proj): Linear(in_features=48, out_features=512, bias=False)\n",
       "        (norm_layer): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (modality_trunks): ModuleDict(\n",
       "    (vision): SimpleTransformer(\n",
       "      (pre_transformer_layer): Sequential(\n",
       "        (0): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): EinOpsRearrange()\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (12): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (13): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (14): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (15): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (16): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (17): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (18): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (19): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (20): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (21): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (22): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (23): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (24): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (25): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (26): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (27): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (28): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (29): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (30): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (31): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (post_transformer_layer): EinOpsRearrange()\n",
       "    )\n",
       "    (text): SimpleTransformer(\n",
       "      (pre_transformer_layer): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): EinOpsRearrange()\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (12): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (13): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (14): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (15): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (16): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (17): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (18): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (19): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (20): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (21): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (22): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (23): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (post_transformer_layer): EinOpsRearrange()\n",
       "    )\n",
       "    (audio): SimpleTransformer(\n",
       "      (pre_transformer_layer): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): EinOpsRearrange()\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.009)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.018)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.027)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.036)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.045)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.055)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.064)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.073)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.082)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.091)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (post_transformer_layer): EinOpsRearrange()\n",
       "    )\n",
       "    (depth): SimpleTransformer(\n",
       "      (pre_transformer_layer): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): EinOpsRearrange()\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (post_transformer_layer): EinOpsRearrange()\n",
       "    )\n",
       "    (thermal): SimpleTransformer(\n",
       "      (pre_transformer_layer): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): EinOpsRearrange()\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (post_transformer_layer): EinOpsRearrange()\n",
       "    )\n",
       "    (imu): SimpleTransformer(\n",
       "      (pre_transformer_layer): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): EinOpsRearrange()\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.140)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.280)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.420)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.560)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.700)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (post_transformer_layer): EinOpsRearrange()\n",
       "    )\n",
       "  )\n",
       "  (modality_heads): ModuleDict(\n",
       "    (vision): Sequential(\n",
       "      (0): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): SelectElement()\n",
       "      (2): Linear(in_features=1280, out_features=1024, bias=False)\n",
       "    )\n",
       "    (text): SelectEOSAndProject(\n",
       "      (proj): Sequential(\n",
       "        (0): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (audio): Sequential(\n",
       "      (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): SelectElement()\n",
       "      (2): Linear(in_features=768, out_features=1024, bias=False)\n",
       "    )\n",
       "    (depth): Sequential(\n",
       "      (0): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): SelectElement()\n",
       "      (2): Linear(in_features=384, out_features=1024, bias=False)\n",
       "    )\n",
       "    (thermal): Sequential(\n",
       "      (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): SelectElement()\n",
       "      (2): Linear(in_features=768, out_features=1024, bias=False)\n",
       "    )\n",
       "    (imu): Sequential(\n",
       "      (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): SelectElement()\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=512, out_features=1024, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (modality_postprocessors): ModuleDict(\n",
       "    (vision): Normalize()\n",
       "    (text): Sequential(\n",
       "      (0): Normalize()\n",
       "      (1): LearnableLogitScaling(logit_scale_init=14.285714285714285,learnable=True, max_logit_scale=100)\n",
       "    )\n",
       "    (audio): Sequential(\n",
       "      (0): Normalize()\n",
       "      (1): LearnableLogitScaling(logit_scale_init=20.0,learnable=False, max_logit_scale=100)\n",
       "    )\n",
       "    (depth): Sequential(\n",
       "      (0): Normalize()\n",
       "      (1): LearnableLogitScaling(logit_scale_init=5.0,learnable=False, max_logit_scale=100)\n",
       "    )\n",
       "    (thermal): Sequential(\n",
       "      (0): Normalize()\n",
       "      (1): LearnableLogitScaling(logit_scale_init=10.0,learnable=False, max_logit_scale=100)\n",
       "    )\n",
       "    (imu): Sequential(\n",
       "      (0): Normalize()\n",
       "      (1): LearnableLogitScaling(logit_scale_init=5.0,learnable=False, max_logit_scale=100)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets.VideoDataset import VideoDataset\n",
    "from models import imagebind_model\n",
    "import torch\n",
    "imagebind=imagebind_model.imagebind_huge(pretrained=True)\n",
    "imagebind.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1024])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dummy_input = torch.zeros(\n",
    "                [\n",
    "                    3\n",
    "                ]\n",
    "                + [3, 2,224, 224]\n",
    "            )\n",
    "dummy_input = dummy_input.unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    embeddings = imagebind({\"vision\":dummy_input})\n",
    "embeddings['vision'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/imagebind/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from train_baseline import VideoTrain\n",
    "from datasets.VideoDataset import VideoDataModule\n",
    "\n",
    "train=VideoTrain()\n",
    "# load batch\n",
    "dataset=VideoDataModule(csv_path='/tsukimi/datasets/Chiba/baseline/datalist_3')\n",
    "loader=dataset.train_dataloader()\n",
    "batch,labels=next(iter(loader))\n",
    "embed=train(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0780,  0.0870, -0.0244]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/imagebind/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'world_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m module\u001b[38;5;241m=\u001b[39mVideoDataModule(csv_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/tsukimi/datasets/Chiba/baseline/datalist_3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m module\u001b[38;5;241m.\u001b[39msetup(stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m loader\u001b[38;5;241m=\u001b[39m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(loader))\n\u001b[1;32m      6\u001b[0m batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(loader))\n",
      "File \u001b[0;32m~/Events-ImageBind/datasets/VideoDataset.py:147\u001b[0m, in \u001b[0;36mVideoDataModule.train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 147\u001b[0m     num_tasks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworld_size\u001b[49m\n\u001b[1;32m    148\u001b[0m     global_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mglobal_rank\n\u001b[1;32m    149\u001b[0m     weighted_sampler\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mWeightedRandomSampler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights,\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'world_size'"
     ]
    }
   ],
   "source": [
    "from datasets.VideoDataset import VideoDataModule\n",
    "module=VideoDataModule(csv_path='/tsukimi/datasets/Chiba/baseline/datalist_3')\n",
    "module.setup(stage='fit')\n",
    "loader=module.train_dataloader()\n",
    "print(len(loader))\n",
    "batch=next(iter(loader))\n",
    "# from train_baseline import VideoTrain\n",
    "# train=VideoTrain()\n",
    "# embed=train(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0755, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from eventutils import AccMetric,ConfusionMatrixMetric, multi_label_accuracy, custom_multi_label_pred, ground_truth_decoder\n",
    "from pytorch_loss import FocalLossV3\n",
    "labels=batch[1]\n",
    "gt=ground_truth_decoder(labels)\n",
    "c=FocalLossV3()\n",
    "c(embed.to('cuda'),gt.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 42, 3, 2, 224, 224])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "next(iter(loader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/imagebind/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers', 'hparams_name', 'hyper_parameters'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "checkpoint_path='/tsukimi/datasets/Chiba/imagebind_baseline/checkpoint/imagebind-epoch=07-val_acc=0.62.ckpt'\n",
    "checkpoint=torch.load(checkpoint_path)\n",
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['imagebind.modality_preprocessors.vision.cls_token', 'imagebind.modality_preprocessors.vision.rgbt_stem.proj.1.weight', 'imagebind.modality_preprocessors.vision.pos_embedding_helper.pos_embed', 'imagebind.modality_preprocessors.text.pos_embed', 'imagebind.modality_preprocessors.text.mask', 'imagebind.modality_preprocessors.text.token_embedding.weight', 'imagebind.modality_preprocessors.audio.cls_token', 'imagebind.modality_preprocessors.audio.rgbt_stem.proj.weight', 'imagebind.modality_preprocessors.audio.rgbt_stem.norm_layer.weight', 'imagebind.modality_preprocessors.audio.rgbt_stem.norm_layer.bias', 'imagebind.modality_preprocessors.audio.pos_embedding_helper.pos_embed', 'imagebind.modality_preprocessors.depth.cls_token', 'imagebind.modality_preprocessors.depth.depth_stem.proj.weight', 'imagebind.modality_preprocessors.depth.depth_stem.norm_layer.weight', 'imagebind.modality_preprocessors.depth.depth_stem.norm_layer.bias', 'imagebind.modality_preprocessors.depth.pos_embedding_helper.pos_embed', 'imagebind.modality_preprocessors.thermal.cls_token', 'imagebind.modality_preprocessors.thermal.rgbt_stem.proj.weight', 'imagebind.modality_preprocessors.thermal.rgbt_stem.norm_layer.weight', 'imagebind.modality_preprocessors.thermal.rgbt_stem.norm_layer.bias', 'imagebind.modality_preprocessors.thermal.pos_embedding_helper.pos_embed', 'imagebind.modality_preprocessors.imu.pos_embed', 'imagebind.modality_preprocessors.imu.cls_token', 'imagebind.modality_preprocessors.imu.imu_stem.proj.weight', 'imagebind.modality_preprocessors.imu.imu_stem.norm_layer.weight', 'imagebind.modality_preprocessors.imu.imu_stem.norm_layer.bias', 'imagebind.modality_trunks.vision.pre_transformer_layer.0.weight', 'imagebind.modality_trunks.vision.pre_transformer_layer.0.bias', 'imagebind.modality_trunks.vision.blocks.0.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.0.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.0.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.0.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.0.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.0.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.0.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.0.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.0.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.0.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.0.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.0.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.1.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.1.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.1.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.1.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.1.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.1.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.1.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.1.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.1.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.1.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.1.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.1.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.2.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.2.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.2.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.2.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.2.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.2.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.2.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.2.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.2.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.2.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.2.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.2.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.3.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.3.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.3.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.3.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.3.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.3.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.3.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.3.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.3.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.3.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.3.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.3.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.4.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.4.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.4.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.4.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.4.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.4.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.4.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.4.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.4.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.4.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.4.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.4.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.5.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.5.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.5.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.5.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.5.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.5.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.5.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.5.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.5.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.5.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.5.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.5.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.6.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.6.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.6.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.6.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.6.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.6.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.6.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.6.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.6.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.6.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.6.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.6.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.7.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.7.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.7.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.7.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.7.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.7.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.7.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.7.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.7.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.7.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.7.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.7.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.8.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.8.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.8.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.8.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.8.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.8.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.8.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.8.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.8.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.8.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.8.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.8.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.9.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.9.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.9.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.9.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.9.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.9.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.9.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.9.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.9.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.9.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.9.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.9.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.10.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.10.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.10.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.10.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.10.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.10.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.10.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.10.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.10.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.10.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.10.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.10.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.11.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.11.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.11.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.11.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.11.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.11.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.11.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.11.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.11.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.11.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.11.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.11.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.12.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.12.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.12.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.12.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.12.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.12.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.12.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.12.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.12.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.12.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.12.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.12.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.13.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.13.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.13.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.13.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.13.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.13.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.13.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.13.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.13.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.13.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.13.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.13.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.14.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.14.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.14.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.14.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.14.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.14.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.14.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.14.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.14.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.14.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.14.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.14.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.15.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.15.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.15.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.15.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.15.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.15.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.15.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.15.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.15.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.15.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.15.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.15.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.16.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.16.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.16.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.16.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.16.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.16.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.16.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.16.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.16.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.16.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.16.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.16.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.17.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.17.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.17.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.17.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.17.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.17.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.17.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.17.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.17.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.17.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.17.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.17.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.18.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.18.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.18.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.18.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.18.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.18.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.18.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.18.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.18.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.18.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.18.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.18.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.19.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.19.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.19.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.19.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.19.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.19.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.19.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.19.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.19.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.19.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.19.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.19.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.20.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.20.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.20.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.20.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.20.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.20.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.20.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.20.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.20.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.20.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.20.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.20.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.21.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.21.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.21.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.21.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.21.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.21.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.21.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.21.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.21.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.21.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.21.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.21.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.22.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.22.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.22.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.22.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.22.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.22.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.22.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.22.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.22.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.22.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.22.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.22.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.23.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.23.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.23.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.23.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.23.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.23.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.23.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.23.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.23.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.23.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.23.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.23.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.24.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.24.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.24.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.24.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.24.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.24.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.24.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.24.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.24.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.24.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.24.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.24.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.25.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.25.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.25.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.25.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.25.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.25.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.25.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.25.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.25.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.25.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.25.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.25.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.26.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.26.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.26.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.26.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.26.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.26.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.26.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.26.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.26.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.26.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.26.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.26.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.27.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.27.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.27.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.27.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.27.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.27.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.27.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.27.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.27.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.27.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.27.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.27.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.28.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.28.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.28.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.28.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.28.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.28.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.28.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.28.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.28.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.28.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.28.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.28.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.29.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.29.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.29.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.29.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.29.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.29.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.29.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.29.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.29.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.29.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.29.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.29.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.30.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.30.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.30.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.30.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.30.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.30.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.30.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.30.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.30.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.30.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.30.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.30.norm_2.bias', 'imagebind.modality_trunks.vision.blocks.31.attn.in_proj_weight', 'imagebind.modality_trunks.vision.blocks.31.attn.in_proj_bias', 'imagebind.modality_trunks.vision.blocks.31.attn.out_proj.weight', 'imagebind.modality_trunks.vision.blocks.31.attn.out_proj.bias', 'imagebind.modality_trunks.vision.blocks.31.norm_1.weight', 'imagebind.modality_trunks.vision.blocks.31.norm_1.bias', 'imagebind.modality_trunks.vision.blocks.31.mlp.fc1.weight', 'imagebind.modality_trunks.vision.blocks.31.mlp.fc1.bias', 'imagebind.modality_trunks.vision.blocks.31.mlp.fc2.weight', 'imagebind.modality_trunks.vision.blocks.31.mlp.fc2.bias', 'imagebind.modality_trunks.vision.blocks.31.norm_2.weight', 'imagebind.modality_trunks.vision.blocks.31.norm_2.bias', 'imagebind.modality_trunks.text.blocks.0.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.0.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.0.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.0.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.0.norm_1.weight', 'imagebind.modality_trunks.text.blocks.0.norm_1.bias', 'imagebind.modality_trunks.text.blocks.0.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.0.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.0.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.0.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.0.norm_2.weight', 'imagebind.modality_trunks.text.blocks.0.norm_2.bias', 'imagebind.modality_trunks.text.blocks.1.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.1.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.1.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.1.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.1.norm_1.weight', 'imagebind.modality_trunks.text.blocks.1.norm_1.bias', 'imagebind.modality_trunks.text.blocks.1.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.1.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.1.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.1.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.1.norm_2.weight', 'imagebind.modality_trunks.text.blocks.1.norm_2.bias', 'imagebind.modality_trunks.text.blocks.2.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.2.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.2.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.2.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.2.norm_1.weight', 'imagebind.modality_trunks.text.blocks.2.norm_1.bias', 'imagebind.modality_trunks.text.blocks.2.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.2.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.2.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.2.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.2.norm_2.weight', 'imagebind.modality_trunks.text.blocks.2.norm_2.bias', 'imagebind.modality_trunks.text.blocks.3.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.3.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.3.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.3.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.3.norm_1.weight', 'imagebind.modality_trunks.text.blocks.3.norm_1.bias', 'imagebind.modality_trunks.text.blocks.3.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.3.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.3.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.3.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.3.norm_2.weight', 'imagebind.modality_trunks.text.blocks.3.norm_2.bias', 'imagebind.modality_trunks.text.blocks.4.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.4.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.4.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.4.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.4.norm_1.weight', 'imagebind.modality_trunks.text.blocks.4.norm_1.bias', 'imagebind.modality_trunks.text.blocks.4.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.4.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.4.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.4.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.4.norm_2.weight', 'imagebind.modality_trunks.text.blocks.4.norm_2.bias', 'imagebind.modality_trunks.text.blocks.5.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.5.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.5.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.5.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.5.norm_1.weight', 'imagebind.modality_trunks.text.blocks.5.norm_1.bias', 'imagebind.modality_trunks.text.blocks.5.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.5.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.5.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.5.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.5.norm_2.weight', 'imagebind.modality_trunks.text.blocks.5.norm_2.bias', 'imagebind.modality_trunks.text.blocks.6.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.6.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.6.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.6.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.6.norm_1.weight', 'imagebind.modality_trunks.text.blocks.6.norm_1.bias', 'imagebind.modality_trunks.text.blocks.6.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.6.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.6.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.6.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.6.norm_2.weight', 'imagebind.modality_trunks.text.blocks.6.norm_2.bias', 'imagebind.modality_trunks.text.blocks.7.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.7.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.7.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.7.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.7.norm_1.weight', 'imagebind.modality_trunks.text.blocks.7.norm_1.bias', 'imagebind.modality_trunks.text.blocks.7.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.7.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.7.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.7.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.7.norm_2.weight', 'imagebind.modality_trunks.text.blocks.7.norm_2.bias', 'imagebind.modality_trunks.text.blocks.8.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.8.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.8.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.8.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.8.norm_1.weight', 'imagebind.modality_trunks.text.blocks.8.norm_1.bias', 'imagebind.modality_trunks.text.blocks.8.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.8.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.8.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.8.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.8.norm_2.weight', 'imagebind.modality_trunks.text.blocks.8.norm_2.bias', 'imagebind.modality_trunks.text.blocks.9.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.9.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.9.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.9.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.9.norm_1.weight', 'imagebind.modality_trunks.text.blocks.9.norm_1.bias', 'imagebind.modality_trunks.text.blocks.9.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.9.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.9.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.9.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.9.norm_2.weight', 'imagebind.modality_trunks.text.blocks.9.norm_2.bias', 'imagebind.modality_trunks.text.blocks.10.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.10.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.10.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.10.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.10.norm_1.weight', 'imagebind.modality_trunks.text.blocks.10.norm_1.bias', 'imagebind.modality_trunks.text.blocks.10.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.10.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.10.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.10.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.10.norm_2.weight', 'imagebind.modality_trunks.text.blocks.10.norm_2.bias', 'imagebind.modality_trunks.text.blocks.11.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.11.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.11.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.11.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.11.norm_1.weight', 'imagebind.modality_trunks.text.blocks.11.norm_1.bias', 'imagebind.modality_trunks.text.blocks.11.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.11.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.11.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.11.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.11.norm_2.weight', 'imagebind.modality_trunks.text.blocks.11.norm_2.bias', 'imagebind.modality_trunks.text.blocks.12.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.12.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.12.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.12.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.12.norm_1.weight', 'imagebind.modality_trunks.text.blocks.12.norm_1.bias', 'imagebind.modality_trunks.text.blocks.12.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.12.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.12.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.12.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.12.norm_2.weight', 'imagebind.modality_trunks.text.blocks.12.norm_2.bias', 'imagebind.modality_trunks.text.blocks.13.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.13.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.13.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.13.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.13.norm_1.weight', 'imagebind.modality_trunks.text.blocks.13.norm_1.bias', 'imagebind.modality_trunks.text.blocks.13.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.13.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.13.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.13.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.13.norm_2.weight', 'imagebind.modality_trunks.text.blocks.13.norm_2.bias', 'imagebind.modality_trunks.text.blocks.14.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.14.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.14.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.14.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.14.norm_1.weight', 'imagebind.modality_trunks.text.blocks.14.norm_1.bias', 'imagebind.modality_trunks.text.blocks.14.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.14.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.14.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.14.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.14.norm_2.weight', 'imagebind.modality_trunks.text.blocks.14.norm_2.bias', 'imagebind.modality_trunks.text.blocks.15.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.15.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.15.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.15.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.15.norm_1.weight', 'imagebind.modality_trunks.text.blocks.15.norm_1.bias', 'imagebind.modality_trunks.text.blocks.15.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.15.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.15.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.15.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.15.norm_2.weight', 'imagebind.modality_trunks.text.blocks.15.norm_2.bias', 'imagebind.modality_trunks.text.blocks.16.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.16.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.16.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.16.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.16.norm_1.weight', 'imagebind.modality_trunks.text.blocks.16.norm_1.bias', 'imagebind.modality_trunks.text.blocks.16.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.16.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.16.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.16.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.16.norm_2.weight', 'imagebind.modality_trunks.text.blocks.16.norm_2.bias', 'imagebind.modality_trunks.text.blocks.17.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.17.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.17.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.17.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.17.norm_1.weight', 'imagebind.modality_trunks.text.blocks.17.norm_1.bias', 'imagebind.modality_trunks.text.blocks.17.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.17.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.17.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.17.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.17.norm_2.weight', 'imagebind.modality_trunks.text.blocks.17.norm_2.bias', 'imagebind.modality_trunks.text.blocks.18.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.18.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.18.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.18.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.18.norm_1.weight', 'imagebind.modality_trunks.text.blocks.18.norm_1.bias', 'imagebind.modality_trunks.text.blocks.18.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.18.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.18.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.18.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.18.norm_2.weight', 'imagebind.modality_trunks.text.blocks.18.norm_2.bias', 'imagebind.modality_trunks.text.blocks.19.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.19.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.19.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.19.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.19.norm_1.weight', 'imagebind.modality_trunks.text.blocks.19.norm_1.bias', 'imagebind.modality_trunks.text.blocks.19.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.19.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.19.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.19.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.19.norm_2.weight', 'imagebind.modality_trunks.text.blocks.19.norm_2.bias', 'imagebind.modality_trunks.text.blocks.20.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.20.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.20.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.20.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.20.norm_1.weight', 'imagebind.modality_trunks.text.blocks.20.norm_1.bias', 'imagebind.modality_trunks.text.blocks.20.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.20.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.20.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.20.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.20.norm_2.weight', 'imagebind.modality_trunks.text.blocks.20.norm_2.bias', 'imagebind.modality_trunks.text.blocks.21.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.21.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.21.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.21.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.21.norm_1.weight', 'imagebind.modality_trunks.text.blocks.21.norm_1.bias', 'imagebind.modality_trunks.text.blocks.21.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.21.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.21.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.21.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.21.norm_2.weight', 'imagebind.modality_trunks.text.blocks.21.norm_2.bias', 'imagebind.modality_trunks.text.blocks.22.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.22.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.22.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.22.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.22.norm_1.weight', 'imagebind.modality_trunks.text.blocks.22.norm_1.bias', 'imagebind.modality_trunks.text.blocks.22.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.22.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.22.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.22.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.22.norm_2.weight', 'imagebind.modality_trunks.text.blocks.22.norm_2.bias', 'imagebind.modality_trunks.text.blocks.23.attn.in_proj_weight', 'imagebind.modality_trunks.text.blocks.23.attn.in_proj_bias', 'imagebind.modality_trunks.text.blocks.23.attn.out_proj.weight', 'imagebind.modality_trunks.text.blocks.23.attn.out_proj.bias', 'imagebind.modality_trunks.text.blocks.23.norm_1.weight', 'imagebind.modality_trunks.text.blocks.23.norm_1.bias', 'imagebind.modality_trunks.text.blocks.23.mlp.fc1.weight', 'imagebind.modality_trunks.text.blocks.23.mlp.fc1.bias', 'imagebind.modality_trunks.text.blocks.23.mlp.fc2.weight', 'imagebind.modality_trunks.text.blocks.23.mlp.fc2.bias', 'imagebind.modality_trunks.text.blocks.23.norm_2.weight', 'imagebind.modality_trunks.text.blocks.23.norm_2.bias', 'imagebind.modality_trunks.audio.blocks.0.attn.in_proj_weight', 'imagebind.modality_trunks.audio.blocks.0.attn.in_proj_bias', 'imagebind.modality_trunks.audio.blocks.0.attn.bias_k', 'imagebind.modality_trunks.audio.blocks.0.attn.bias_v', 'imagebind.modality_trunks.audio.blocks.0.attn.out_proj.weight', 'imagebind.modality_trunks.audio.blocks.0.attn.out_proj.bias', 'imagebind.modality_trunks.audio.blocks.0.norm_1.weight', 'imagebind.modality_trunks.audio.blocks.0.norm_1.bias', 'imagebind.modality_trunks.audio.blocks.0.mlp.fc1.weight', 'imagebind.modality_trunks.audio.blocks.0.mlp.fc1.bias', 'imagebind.modality_trunks.audio.blocks.0.mlp.fc2.weight', 'imagebind.modality_trunks.audio.blocks.0.mlp.fc2.bias', 'imagebind.modality_trunks.audio.blocks.0.norm_2.weight', 'imagebind.modality_trunks.audio.blocks.0.norm_2.bias', 'imagebind.modality_trunks.audio.blocks.1.attn.in_proj_weight', 'imagebind.modality_trunks.audio.blocks.1.attn.in_proj_bias', 'imagebind.modality_trunks.audio.blocks.1.attn.bias_k', 'imagebind.modality_trunks.audio.blocks.1.attn.bias_v', 'imagebind.modality_trunks.audio.blocks.1.attn.out_proj.weight', 'imagebind.modality_trunks.audio.blocks.1.attn.out_proj.bias', 'imagebind.modality_trunks.audio.blocks.1.norm_1.weight', 'imagebind.modality_trunks.audio.blocks.1.norm_1.bias', 'imagebind.modality_trunks.audio.blocks.1.mlp.fc1.weight', 'imagebind.modality_trunks.audio.blocks.1.mlp.fc1.bias', 'imagebind.modality_trunks.audio.blocks.1.mlp.fc2.weight', 'imagebind.modality_trunks.audio.blocks.1.mlp.fc2.bias', 'imagebind.modality_trunks.audio.blocks.1.norm_2.weight', 'imagebind.modality_trunks.audio.blocks.1.norm_2.bias', 'imagebind.modality_trunks.audio.blocks.2.attn.in_proj_weight', 'imagebind.modality_trunks.audio.blocks.2.attn.in_proj_bias', 'imagebind.modality_trunks.audio.blocks.2.attn.bias_k', 'imagebind.modality_trunks.audio.blocks.2.attn.bias_v', 'imagebind.modality_trunks.audio.blocks.2.attn.out_proj.weight', 'imagebind.modality_trunks.audio.blocks.2.attn.out_proj.bias', 'imagebind.modality_trunks.audio.blocks.2.norm_1.weight', 'imagebind.modality_trunks.audio.blocks.2.norm_1.bias', 'imagebind.modality_trunks.audio.blocks.2.mlp.fc1.weight', 'imagebind.modality_trunks.audio.blocks.2.mlp.fc1.bias', 'imagebind.modality_trunks.audio.blocks.2.mlp.fc2.weight', 'imagebind.modality_trunks.audio.blocks.2.mlp.fc2.bias', 'imagebind.modality_trunks.audio.blocks.2.norm_2.weight', 'imagebind.modality_trunks.audio.blocks.2.norm_2.bias', 'imagebind.modality_trunks.audio.blocks.3.attn.in_proj_weight', 'imagebind.modality_trunks.audio.blocks.3.attn.in_proj_bias', 'imagebind.modality_trunks.audio.blocks.3.attn.bias_k', 'imagebind.modality_trunks.audio.blocks.3.attn.bias_v', 'imagebind.modality_trunks.audio.blocks.3.attn.out_proj.weight', 'imagebind.modality_trunks.audio.blocks.3.attn.out_proj.bias', 'imagebind.modality_trunks.audio.blocks.3.norm_1.weight', 'imagebind.modality_trunks.audio.blocks.3.norm_1.bias', 'imagebind.modality_trunks.audio.blocks.3.mlp.fc1.weight', 'imagebind.modality_trunks.audio.blocks.3.mlp.fc1.bias', 'imagebind.modality_trunks.audio.blocks.3.mlp.fc2.weight', 'imagebind.modality_trunks.audio.blocks.3.mlp.fc2.bias', 'imagebind.modality_trunks.audio.blocks.3.norm_2.weight', 'imagebind.modality_trunks.audio.blocks.3.norm_2.bias', 'imagebind.modality_trunks.audio.blocks.4.attn.in_proj_weight', 'imagebind.modality_trunks.audio.blocks.4.attn.in_proj_bias', 'imagebind.modality_trunks.audio.blocks.4.attn.bias_k', 'imagebind.modality_trunks.audio.blocks.4.attn.bias_v', 'imagebind.modality_trunks.audio.blocks.4.attn.out_proj.weight', 'imagebind.modality_trunks.audio.blocks.4.attn.out_proj.bias', 'imagebind.modality_trunks.audio.blocks.4.norm_1.weight', 'imagebind.modality_trunks.audio.blocks.4.norm_1.bias', 'imagebind.modality_trunks.audio.blocks.4.mlp.fc1.weight', 'imagebind.modality_trunks.audio.blocks.4.mlp.fc1.bias', 'imagebind.modality_trunks.audio.blocks.4.mlp.fc2.weight', 'imagebind.modality_trunks.audio.blocks.4.mlp.fc2.bias', 'imagebind.modality_trunks.audio.blocks.4.norm_2.weight', 'imagebind.modality_trunks.audio.blocks.4.norm_2.bias', 'imagebind.modality_trunks.audio.blocks.5.attn.in_proj_weight', 'imagebind.modality_trunks.audio.blocks.5.attn.in_proj_bias', 'imagebind.modality_trunks.audio.blocks.5.attn.bias_k', 'imagebind.modality_trunks.audio.blocks.5.attn.bias_v', 'imagebind.modality_trunks.audio.blocks.5.attn.out_proj.weight', 'imagebind.modality_trunks.audio.blocks.5.attn.out_proj.bias', 'imagebind.modality_trunks.audio.blocks.5.norm_1.weight', 'imagebind.modality_trunks.audio.blocks.5.norm_1.bias', 'imagebind.modality_trunks.audio.blocks.5.mlp.fc1.weight', 'imagebind.modality_trunks.audio.blocks.5.mlp.fc1.bias', 'imagebind.modality_trunks.audio.blocks.5.mlp.fc2.weight', 'imagebind.modality_trunks.audio.blocks.5.mlp.fc2.bias', 'imagebind.modality_trunks.audio.blocks.5.norm_2.weight', 'imagebind.modality_trunks.audio.blocks.5.norm_2.bias', 'imagebind.modality_trunks.audio.blocks.6.attn.in_proj_weight', 'imagebind.modality_trunks.audio.blocks.6.attn.in_proj_bias', 'imagebind.modality_trunks.audio.blocks.6.attn.bias_k', 'imagebind.modality_trunks.audio.blocks.6.attn.bias_v', 'imagebind.modality_trunks.audio.blocks.6.attn.out_proj.weight', 'imagebind.modality_trunks.audio.blocks.6.attn.out_proj.bias', 'imagebind.modality_trunks.audio.blocks.6.norm_1.weight', 'imagebind.modality_trunks.audio.blocks.6.norm_1.bias', 'imagebind.modality_trunks.audio.blocks.6.mlp.fc1.weight', 'imagebind.modality_trunks.audio.blocks.6.mlp.fc1.bias', 'imagebind.modality_trunks.audio.blocks.6.mlp.fc2.weight', 'imagebind.modality_trunks.audio.blocks.6.mlp.fc2.bias', 'imagebind.modality_trunks.audio.blocks.6.norm_2.weight', 'imagebind.modality_trunks.audio.blocks.6.norm_2.bias', 'imagebind.modality_trunks.audio.blocks.7.attn.in_proj_weight', 'imagebind.modality_trunks.audio.blocks.7.attn.in_proj_bias', 'imagebind.modality_trunks.audio.blocks.7.attn.bias_k', 'imagebind.modality_trunks.audio.blocks.7.attn.bias_v', 'imagebind.modality_trunks.audio.blocks.7.attn.out_proj.weight', 'imagebind.modality_trunks.audio.blocks.7.attn.out_proj.bias', 'imagebind.modality_trunks.audio.blocks.7.norm_1.weight', 'imagebind.modality_trunks.audio.blocks.7.norm_1.bias', 'imagebind.modality_trunks.audio.blocks.7.mlp.fc1.weight', 'imagebind.modality_trunks.audio.blocks.7.mlp.fc1.bias', 'imagebind.modality_trunks.audio.blocks.7.mlp.fc2.weight', 'imagebind.modality_trunks.audio.blocks.7.mlp.fc2.bias', 'imagebind.modality_trunks.audio.blocks.7.norm_2.weight', 'imagebind.modality_trunks.audio.blocks.7.norm_2.bias', 'imagebind.modality_trunks.audio.blocks.8.attn.in_proj_weight', 'imagebind.modality_trunks.audio.blocks.8.attn.in_proj_bias', 'imagebind.modality_trunks.audio.blocks.8.attn.bias_k', 'imagebind.modality_trunks.audio.blocks.8.attn.bias_v', 'imagebind.modality_trunks.audio.blocks.8.attn.out_proj.weight', 'imagebind.modality_trunks.audio.blocks.8.attn.out_proj.bias', 'imagebind.modality_trunks.audio.blocks.8.norm_1.weight', 'imagebind.modality_trunks.audio.blocks.8.norm_1.bias', 'imagebind.modality_trunks.audio.blocks.8.mlp.fc1.weight', 'imagebind.modality_trunks.audio.blocks.8.mlp.fc1.bias', 'imagebind.modality_trunks.audio.blocks.8.mlp.fc2.weight', 'imagebind.modality_trunks.audio.blocks.8.mlp.fc2.bias', 'imagebind.modality_trunks.audio.blocks.8.norm_2.weight', 'imagebind.modality_trunks.audio.blocks.8.norm_2.bias', 'imagebind.modality_trunks.audio.blocks.9.attn.in_proj_weight', 'imagebind.modality_trunks.audio.blocks.9.attn.in_proj_bias', 'imagebind.modality_trunks.audio.blocks.9.attn.bias_k', 'imagebind.modality_trunks.audio.blocks.9.attn.bias_v', 'imagebind.modality_trunks.audio.blocks.9.attn.out_proj.weight', 'imagebind.modality_trunks.audio.blocks.9.attn.out_proj.bias', 'imagebind.modality_trunks.audio.blocks.9.norm_1.weight', 'imagebind.modality_trunks.audio.blocks.9.norm_1.bias', 'imagebind.modality_trunks.audio.blocks.9.mlp.fc1.weight', 'imagebind.modality_trunks.audio.blocks.9.mlp.fc1.bias', 'imagebind.modality_trunks.audio.blocks.9.mlp.fc2.weight', 'imagebind.modality_trunks.audio.blocks.9.mlp.fc2.bias', 'imagebind.modality_trunks.audio.blocks.9.norm_2.weight', 'imagebind.modality_trunks.audio.blocks.9.norm_2.bias', 'imagebind.modality_trunks.audio.blocks.10.attn.in_proj_weight', 'imagebind.modality_trunks.audio.blocks.10.attn.in_proj_bias', 'imagebind.modality_trunks.audio.blocks.10.attn.bias_k', 'imagebind.modality_trunks.audio.blocks.10.attn.bias_v', 'imagebind.modality_trunks.audio.blocks.10.attn.out_proj.weight', 'imagebind.modality_trunks.audio.blocks.10.attn.out_proj.bias', 'imagebind.modality_trunks.audio.blocks.10.norm_1.weight', 'imagebind.modality_trunks.audio.blocks.10.norm_1.bias', 'imagebind.modality_trunks.audio.blocks.10.mlp.fc1.weight', 'imagebind.modality_trunks.audio.blocks.10.mlp.fc1.bias', 'imagebind.modality_trunks.audio.blocks.10.mlp.fc2.weight', 'imagebind.modality_trunks.audio.blocks.10.mlp.fc2.bias', 'imagebind.modality_trunks.audio.blocks.10.norm_2.weight', 'imagebind.modality_trunks.audio.blocks.10.norm_2.bias', 'imagebind.modality_trunks.audio.blocks.11.attn.in_proj_weight', 'imagebind.modality_trunks.audio.blocks.11.attn.in_proj_bias', 'imagebind.modality_trunks.audio.blocks.11.attn.bias_k', 'imagebind.modality_trunks.audio.blocks.11.attn.bias_v', 'imagebind.modality_trunks.audio.blocks.11.attn.out_proj.weight', 'imagebind.modality_trunks.audio.blocks.11.attn.out_proj.bias', 'imagebind.modality_trunks.audio.blocks.11.norm_1.weight', 'imagebind.modality_trunks.audio.blocks.11.norm_1.bias', 'imagebind.modality_trunks.audio.blocks.11.mlp.fc1.weight', 'imagebind.modality_trunks.audio.blocks.11.mlp.fc1.bias', 'imagebind.modality_trunks.audio.blocks.11.mlp.fc2.weight', 'imagebind.modality_trunks.audio.blocks.11.mlp.fc2.bias', 'imagebind.modality_trunks.audio.blocks.11.norm_2.weight', 'imagebind.modality_trunks.audio.blocks.11.norm_2.bias', 'imagebind.modality_trunks.depth.blocks.0.attn.in_proj_weight', 'imagebind.modality_trunks.depth.blocks.0.attn.in_proj_bias', 'imagebind.modality_trunks.depth.blocks.0.attn.bias_k', 'imagebind.modality_trunks.depth.blocks.0.attn.bias_v', 'imagebind.modality_trunks.depth.blocks.0.attn.out_proj.weight', 'imagebind.modality_trunks.depth.blocks.0.attn.out_proj.bias', 'imagebind.modality_trunks.depth.blocks.0.norm_1.weight', 'imagebind.modality_trunks.depth.blocks.0.norm_1.bias', 'imagebind.modality_trunks.depth.blocks.0.mlp.fc1.weight', 'imagebind.modality_trunks.depth.blocks.0.mlp.fc1.bias', 'imagebind.modality_trunks.depth.blocks.0.mlp.fc2.weight', 'imagebind.modality_trunks.depth.blocks.0.mlp.fc2.bias', 'imagebind.modality_trunks.depth.blocks.0.norm_2.weight', 'imagebind.modality_trunks.depth.blocks.0.norm_2.bias', 'imagebind.modality_trunks.depth.blocks.1.attn.in_proj_weight', 'imagebind.modality_trunks.depth.blocks.1.attn.in_proj_bias', 'imagebind.modality_trunks.depth.blocks.1.attn.bias_k', 'imagebind.modality_trunks.depth.blocks.1.attn.bias_v', 'imagebind.modality_trunks.depth.blocks.1.attn.out_proj.weight', 'imagebind.modality_trunks.depth.blocks.1.attn.out_proj.bias', 'imagebind.modality_trunks.depth.blocks.1.norm_1.weight', 'imagebind.modality_trunks.depth.blocks.1.norm_1.bias', 'imagebind.modality_trunks.depth.blocks.1.mlp.fc1.weight', 'imagebind.modality_trunks.depth.blocks.1.mlp.fc1.bias', 'imagebind.modality_trunks.depth.blocks.1.mlp.fc2.weight', 'imagebind.modality_trunks.depth.blocks.1.mlp.fc2.bias', 'imagebind.modality_trunks.depth.blocks.1.norm_2.weight', 'imagebind.modality_trunks.depth.blocks.1.norm_2.bias', 'imagebind.modality_trunks.depth.blocks.2.attn.in_proj_weight', 'imagebind.modality_trunks.depth.blocks.2.attn.in_proj_bias', 'imagebind.modality_trunks.depth.blocks.2.attn.bias_k', 'imagebind.modality_trunks.depth.blocks.2.attn.bias_v', 'imagebind.modality_trunks.depth.blocks.2.attn.out_proj.weight', 'imagebind.modality_trunks.depth.blocks.2.attn.out_proj.bias', 'imagebind.modality_trunks.depth.blocks.2.norm_1.weight', 'imagebind.modality_trunks.depth.blocks.2.norm_1.bias', 'imagebind.modality_trunks.depth.blocks.2.mlp.fc1.weight', 'imagebind.modality_trunks.depth.blocks.2.mlp.fc1.bias', 'imagebind.modality_trunks.depth.blocks.2.mlp.fc2.weight', 'imagebind.modality_trunks.depth.blocks.2.mlp.fc2.bias', 'imagebind.modality_trunks.depth.blocks.2.norm_2.weight', 'imagebind.modality_trunks.depth.blocks.2.norm_2.bias', 'imagebind.modality_trunks.depth.blocks.3.attn.in_proj_weight', 'imagebind.modality_trunks.depth.blocks.3.attn.in_proj_bias', 'imagebind.modality_trunks.depth.blocks.3.attn.bias_k', 'imagebind.modality_trunks.depth.blocks.3.attn.bias_v', 'imagebind.modality_trunks.depth.blocks.3.attn.out_proj.weight', 'imagebind.modality_trunks.depth.blocks.3.attn.out_proj.bias', 'imagebind.modality_trunks.depth.blocks.3.norm_1.weight', 'imagebind.modality_trunks.depth.blocks.3.norm_1.bias', 'imagebind.modality_trunks.depth.blocks.3.mlp.fc1.weight', 'imagebind.modality_trunks.depth.blocks.3.mlp.fc1.bias', 'imagebind.modality_trunks.depth.blocks.3.mlp.fc2.weight', 'imagebind.modality_trunks.depth.blocks.3.mlp.fc2.bias', 'imagebind.modality_trunks.depth.blocks.3.norm_2.weight', 'imagebind.modality_trunks.depth.blocks.3.norm_2.bias', 'imagebind.modality_trunks.depth.blocks.4.attn.in_proj_weight', 'imagebind.modality_trunks.depth.blocks.4.attn.in_proj_bias', 'imagebind.modality_trunks.depth.blocks.4.attn.bias_k', 'imagebind.modality_trunks.depth.blocks.4.attn.bias_v', 'imagebind.modality_trunks.depth.blocks.4.attn.out_proj.weight', 'imagebind.modality_trunks.depth.blocks.4.attn.out_proj.bias', 'imagebind.modality_trunks.depth.blocks.4.norm_1.weight', 'imagebind.modality_trunks.depth.blocks.4.norm_1.bias', 'imagebind.modality_trunks.depth.blocks.4.mlp.fc1.weight', 'imagebind.modality_trunks.depth.blocks.4.mlp.fc1.bias', 'imagebind.modality_trunks.depth.blocks.4.mlp.fc2.weight', 'imagebind.modality_trunks.depth.blocks.4.mlp.fc2.bias', 'imagebind.modality_trunks.depth.blocks.4.norm_2.weight', 'imagebind.modality_trunks.depth.blocks.4.norm_2.bias', 'imagebind.modality_trunks.depth.blocks.5.attn.in_proj_weight', 'imagebind.modality_trunks.depth.blocks.5.attn.in_proj_bias', 'imagebind.modality_trunks.depth.blocks.5.attn.bias_k', 'imagebind.modality_trunks.depth.blocks.5.attn.bias_v', 'imagebind.modality_trunks.depth.blocks.5.attn.out_proj.weight', 'imagebind.modality_trunks.depth.blocks.5.attn.out_proj.bias', 'imagebind.modality_trunks.depth.blocks.5.norm_1.weight', 'imagebind.modality_trunks.depth.blocks.5.norm_1.bias', 'imagebind.modality_trunks.depth.blocks.5.mlp.fc1.weight', 'imagebind.modality_trunks.depth.blocks.5.mlp.fc1.bias', 'imagebind.modality_trunks.depth.blocks.5.mlp.fc2.weight', 'imagebind.modality_trunks.depth.blocks.5.mlp.fc2.bias', 'imagebind.modality_trunks.depth.blocks.5.norm_2.weight', 'imagebind.modality_trunks.depth.blocks.5.norm_2.bias', 'imagebind.modality_trunks.depth.blocks.6.attn.in_proj_weight', 'imagebind.modality_trunks.depth.blocks.6.attn.in_proj_bias', 'imagebind.modality_trunks.depth.blocks.6.attn.bias_k', 'imagebind.modality_trunks.depth.blocks.6.attn.bias_v', 'imagebind.modality_trunks.depth.blocks.6.attn.out_proj.weight', 'imagebind.modality_trunks.depth.blocks.6.attn.out_proj.bias', 'imagebind.modality_trunks.depth.blocks.6.norm_1.weight', 'imagebind.modality_trunks.depth.blocks.6.norm_1.bias', 'imagebind.modality_trunks.depth.blocks.6.mlp.fc1.weight', 'imagebind.modality_trunks.depth.blocks.6.mlp.fc1.bias', 'imagebind.modality_trunks.depth.blocks.6.mlp.fc2.weight', 'imagebind.modality_trunks.depth.blocks.6.mlp.fc2.bias', 'imagebind.modality_trunks.depth.blocks.6.norm_2.weight', 'imagebind.modality_trunks.depth.blocks.6.norm_2.bias', 'imagebind.modality_trunks.depth.blocks.7.attn.in_proj_weight', 'imagebind.modality_trunks.depth.blocks.7.attn.in_proj_bias', 'imagebind.modality_trunks.depth.blocks.7.attn.bias_k', 'imagebind.modality_trunks.depth.blocks.7.attn.bias_v', 'imagebind.modality_trunks.depth.blocks.7.attn.out_proj.weight', 'imagebind.modality_trunks.depth.blocks.7.attn.out_proj.bias', 'imagebind.modality_trunks.depth.blocks.7.norm_1.weight', 'imagebind.modality_trunks.depth.blocks.7.norm_1.bias', 'imagebind.modality_trunks.depth.blocks.7.mlp.fc1.weight', 'imagebind.modality_trunks.depth.blocks.7.mlp.fc1.bias', 'imagebind.modality_trunks.depth.blocks.7.mlp.fc2.weight', 'imagebind.modality_trunks.depth.blocks.7.mlp.fc2.bias', 'imagebind.modality_trunks.depth.blocks.7.norm_2.weight', 'imagebind.modality_trunks.depth.blocks.7.norm_2.bias', 'imagebind.modality_trunks.depth.blocks.8.attn.in_proj_weight', 'imagebind.modality_trunks.depth.blocks.8.attn.in_proj_bias', 'imagebind.modality_trunks.depth.blocks.8.attn.bias_k', 'imagebind.modality_trunks.depth.blocks.8.attn.bias_v', 'imagebind.modality_trunks.depth.blocks.8.attn.out_proj.weight', 'imagebind.modality_trunks.depth.blocks.8.attn.out_proj.bias', 'imagebind.modality_trunks.depth.blocks.8.norm_1.weight', 'imagebind.modality_trunks.depth.blocks.8.norm_1.bias', 'imagebind.modality_trunks.depth.blocks.8.mlp.fc1.weight', 'imagebind.modality_trunks.depth.blocks.8.mlp.fc1.bias', 'imagebind.modality_trunks.depth.blocks.8.mlp.fc2.weight', 'imagebind.modality_trunks.depth.blocks.8.mlp.fc2.bias', 'imagebind.modality_trunks.depth.blocks.8.norm_2.weight', 'imagebind.modality_trunks.depth.blocks.8.norm_2.bias', 'imagebind.modality_trunks.depth.blocks.9.attn.in_proj_weight', 'imagebind.modality_trunks.depth.blocks.9.attn.in_proj_bias', 'imagebind.modality_trunks.depth.blocks.9.attn.bias_k', 'imagebind.modality_trunks.depth.blocks.9.attn.bias_v', 'imagebind.modality_trunks.depth.blocks.9.attn.out_proj.weight', 'imagebind.modality_trunks.depth.blocks.9.attn.out_proj.bias', 'imagebind.modality_trunks.depth.blocks.9.norm_1.weight', 'imagebind.modality_trunks.depth.blocks.9.norm_1.bias', 'imagebind.modality_trunks.depth.blocks.9.mlp.fc1.weight', 'imagebind.modality_trunks.depth.blocks.9.mlp.fc1.bias', 'imagebind.modality_trunks.depth.blocks.9.mlp.fc2.weight', 'imagebind.modality_trunks.depth.blocks.9.mlp.fc2.bias', 'imagebind.modality_trunks.depth.blocks.9.norm_2.weight', 'imagebind.modality_trunks.depth.blocks.9.norm_2.bias', 'imagebind.modality_trunks.depth.blocks.10.attn.in_proj_weight', 'imagebind.modality_trunks.depth.blocks.10.attn.in_proj_bias', 'imagebind.modality_trunks.depth.blocks.10.attn.bias_k', 'imagebind.modality_trunks.depth.blocks.10.attn.bias_v', 'imagebind.modality_trunks.depth.blocks.10.attn.out_proj.weight', 'imagebind.modality_trunks.depth.blocks.10.attn.out_proj.bias', 'imagebind.modality_trunks.depth.blocks.10.norm_1.weight', 'imagebind.modality_trunks.depth.blocks.10.norm_1.bias', 'imagebind.modality_trunks.depth.blocks.10.mlp.fc1.weight', 'imagebind.modality_trunks.depth.blocks.10.mlp.fc1.bias', 'imagebind.modality_trunks.depth.blocks.10.mlp.fc2.weight', 'imagebind.modality_trunks.depth.blocks.10.mlp.fc2.bias', 'imagebind.modality_trunks.depth.blocks.10.norm_2.weight', 'imagebind.modality_trunks.depth.blocks.10.norm_2.bias', 'imagebind.modality_trunks.depth.blocks.11.attn.in_proj_weight', 'imagebind.modality_trunks.depth.blocks.11.attn.in_proj_bias', 'imagebind.modality_trunks.depth.blocks.11.attn.bias_k', 'imagebind.modality_trunks.depth.blocks.11.attn.bias_v', 'imagebind.modality_trunks.depth.blocks.11.attn.out_proj.weight', 'imagebind.modality_trunks.depth.blocks.11.attn.out_proj.bias', 'imagebind.modality_trunks.depth.blocks.11.norm_1.weight', 'imagebind.modality_trunks.depth.blocks.11.norm_1.bias', 'imagebind.modality_trunks.depth.blocks.11.mlp.fc1.weight', 'imagebind.modality_trunks.depth.blocks.11.mlp.fc1.bias', 'imagebind.modality_trunks.depth.blocks.11.mlp.fc2.weight', 'imagebind.modality_trunks.depth.blocks.11.mlp.fc2.bias', 'imagebind.modality_trunks.depth.blocks.11.norm_2.weight', 'imagebind.modality_trunks.depth.blocks.11.norm_2.bias', 'imagebind.modality_trunks.thermal.blocks.0.attn.in_proj_weight', 'imagebind.modality_trunks.thermal.blocks.0.attn.in_proj_bias', 'imagebind.modality_trunks.thermal.blocks.0.attn.bias_k', 'imagebind.modality_trunks.thermal.blocks.0.attn.bias_v', 'imagebind.modality_trunks.thermal.blocks.0.attn.out_proj.weight', 'imagebind.modality_trunks.thermal.blocks.0.attn.out_proj.bias', 'imagebind.modality_trunks.thermal.blocks.0.norm_1.weight', 'imagebind.modality_trunks.thermal.blocks.0.norm_1.bias', 'imagebind.modality_trunks.thermal.blocks.0.mlp.fc1.weight', 'imagebind.modality_trunks.thermal.blocks.0.mlp.fc1.bias', 'imagebind.modality_trunks.thermal.blocks.0.mlp.fc2.weight', 'imagebind.modality_trunks.thermal.blocks.0.mlp.fc2.bias', 'imagebind.modality_trunks.thermal.blocks.0.norm_2.weight', 'imagebind.modality_trunks.thermal.blocks.0.norm_2.bias', 'imagebind.modality_trunks.thermal.blocks.1.attn.in_proj_weight', 'imagebind.modality_trunks.thermal.blocks.1.attn.in_proj_bias', 'imagebind.modality_trunks.thermal.blocks.1.attn.bias_k', 'imagebind.modality_trunks.thermal.blocks.1.attn.bias_v', 'imagebind.modality_trunks.thermal.blocks.1.attn.out_proj.weight', 'imagebind.modality_trunks.thermal.blocks.1.attn.out_proj.bias', 'imagebind.modality_trunks.thermal.blocks.1.norm_1.weight', 'imagebind.modality_trunks.thermal.blocks.1.norm_1.bias', 'imagebind.modality_trunks.thermal.blocks.1.mlp.fc1.weight', 'imagebind.modality_trunks.thermal.blocks.1.mlp.fc1.bias', 'imagebind.modality_trunks.thermal.blocks.1.mlp.fc2.weight', 'imagebind.modality_trunks.thermal.blocks.1.mlp.fc2.bias', 'imagebind.modality_trunks.thermal.blocks.1.norm_2.weight', 'imagebind.modality_trunks.thermal.blocks.1.norm_2.bias', 'imagebind.modality_trunks.thermal.blocks.2.attn.in_proj_weight', 'imagebind.modality_trunks.thermal.blocks.2.attn.in_proj_bias', 'imagebind.modality_trunks.thermal.blocks.2.attn.bias_k', 'imagebind.modality_trunks.thermal.blocks.2.attn.bias_v', 'imagebind.modality_trunks.thermal.blocks.2.attn.out_proj.weight', 'imagebind.modality_trunks.thermal.blocks.2.attn.out_proj.bias', 'imagebind.modality_trunks.thermal.blocks.2.norm_1.weight', 'imagebind.modality_trunks.thermal.blocks.2.norm_1.bias', 'imagebind.modality_trunks.thermal.blocks.2.mlp.fc1.weight', 'imagebind.modality_trunks.thermal.blocks.2.mlp.fc1.bias', 'imagebind.modality_trunks.thermal.blocks.2.mlp.fc2.weight', 'imagebind.modality_trunks.thermal.blocks.2.mlp.fc2.bias', 'imagebind.modality_trunks.thermal.blocks.2.norm_2.weight', 'imagebind.modality_trunks.thermal.blocks.2.norm_2.bias', 'imagebind.modality_trunks.thermal.blocks.3.attn.in_proj_weight', 'imagebind.modality_trunks.thermal.blocks.3.attn.in_proj_bias', 'imagebind.modality_trunks.thermal.blocks.3.attn.bias_k', 'imagebind.modality_trunks.thermal.blocks.3.attn.bias_v', 'imagebind.modality_trunks.thermal.blocks.3.attn.out_proj.weight', 'imagebind.modality_trunks.thermal.blocks.3.attn.out_proj.bias', 'imagebind.modality_trunks.thermal.blocks.3.norm_1.weight', 'imagebind.modality_trunks.thermal.blocks.3.norm_1.bias', 'imagebind.modality_trunks.thermal.blocks.3.mlp.fc1.weight', 'imagebind.modality_trunks.thermal.blocks.3.mlp.fc1.bias', 'imagebind.modality_trunks.thermal.blocks.3.mlp.fc2.weight', 'imagebind.modality_trunks.thermal.blocks.3.mlp.fc2.bias', 'imagebind.modality_trunks.thermal.blocks.3.norm_2.weight', 'imagebind.modality_trunks.thermal.blocks.3.norm_2.bias', 'imagebind.modality_trunks.thermal.blocks.4.attn.in_proj_weight', 'imagebind.modality_trunks.thermal.blocks.4.attn.in_proj_bias', 'imagebind.modality_trunks.thermal.blocks.4.attn.bias_k', 'imagebind.modality_trunks.thermal.blocks.4.attn.bias_v', 'imagebind.modality_trunks.thermal.blocks.4.attn.out_proj.weight', 'imagebind.modality_trunks.thermal.blocks.4.attn.out_proj.bias', 'imagebind.modality_trunks.thermal.blocks.4.norm_1.weight', 'imagebind.modality_trunks.thermal.blocks.4.norm_1.bias', 'imagebind.modality_trunks.thermal.blocks.4.mlp.fc1.weight', 'imagebind.modality_trunks.thermal.blocks.4.mlp.fc1.bias', 'imagebind.modality_trunks.thermal.blocks.4.mlp.fc2.weight', 'imagebind.modality_trunks.thermal.blocks.4.mlp.fc2.bias', 'imagebind.modality_trunks.thermal.blocks.4.norm_2.weight', 'imagebind.modality_trunks.thermal.blocks.4.norm_2.bias', 'imagebind.modality_trunks.thermal.blocks.5.attn.in_proj_weight', 'imagebind.modality_trunks.thermal.blocks.5.attn.in_proj_bias', 'imagebind.modality_trunks.thermal.blocks.5.attn.bias_k', 'imagebind.modality_trunks.thermal.blocks.5.attn.bias_v', 'imagebind.modality_trunks.thermal.blocks.5.attn.out_proj.weight', 'imagebind.modality_trunks.thermal.blocks.5.attn.out_proj.bias', 'imagebind.modality_trunks.thermal.blocks.5.norm_1.weight', 'imagebind.modality_trunks.thermal.blocks.5.norm_1.bias', 'imagebind.modality_trunks.thermal.blocks.5.mlp.fc1.weight', 'imagebind.modality_trunks.thermal.blocks.5.mlp.fc1.bias', 'imagebind.modality_trunks.thermal.blocks.5.mlp.fc2.weight', 'imagebind.modality_trunks.thermal.blocks.5.mlp.fc2.bias', 'imagebind.modality_trunks.thermal.blocks.5.norm_2.weight', 'imagebind.modality_trunks.thermal.blocks.5.norm_2.bias', 'imagebind.modality_trunks.thermal.blocks.6.attn.in_proj_weight', 'imagebind.modality_trunks.thermal.blocks.6.attn.in_proj_bias', 'imagebind.modality_trunks.thermal.blocks.6.attn.bias_k', 'imagebind.modality_trunks.thermal.blocks.6.attn.bias_v', 'imagebind.modality_trunks.thermal.blocks.6.attn.out_proj.weight', 'imagebind.modality_trunks.thermal.blocks.6.attn.out_proj.bias', 'imagebind.modality_trunks.thermal.blocks.6.norm_1.weight', 'imagebind.modality_trunks.thermal.blocks.6.norm_1.bias', 'imagebind.modality_trunks.thermal.blocks.6.mlp.fc1.weight', 'imagebind.modality_trunks.thermal.blocks.6.mlp.fc1.bias', 'imagebind.modality_trunks.thermal.blocks.6.mlp.fc2.weight', 'imagebind.modality_trunks.thermal.blocks.6.mlp.fc2.bias', 'imagebind.modality_trunks.thermal.blocks.6.norm_2.weight', 'imagebind.modality_trunks.thermal.blocks.6.norm_2.bias', 'imagebind.modality_trunks.thermal.blocks.7.attn.in_proj_weight', 'imagebind.modality_trunks.thermal.blocks.7.attn.in_proj_bias', 'imagebind.modality_trunks.thermal.blocks.7.attn.bias_k', 'imagebind.modality_trunks.thermal.blocks.7.attn.bias_v', 'imagebind.modality_trunks.thermal.blocks.7.attn.out_proj.weight', 'imagebind.modality_trunks.thermal.blocks.7.attn.out_proj.bias', 'imagebind.modality_trunks.thermal.blocks.7.norm_1.weight', 'imagebind.modality_trunks.thermal.blocks.7.norm_1.bias', 'imagebind.modality_trunks.thermal.blocks.7.mlp.fc1.weight', 'imagebind.modality_trunks.thermal.blocks.7.mlp.fc1.bias', 'imagebind.modality_trunks.thermal.blocks.7.mlp.fc2.weight', 'imagebind.modality_trunks.thermal.blocks.7.mlp.fc2.bias', 'imagebind.modality_trunks.thermal.blocks.7.norm_2.weight', 'imagebind.modality_trunks.thermal.blocks.7.norm_2.bias', 'imagebind.modality_trunks.thermal.blocks.8.attn.in_proj_weight', 'imagebind.modality_trunks.thermal.blocks.8.attn.in_proj_bias', 'imagebind.modality_trunks.thermal.blocks.8.attn.bias_k', 'imagebind.modality_trunks.thermal.blocks.8.attn.bias_v', 'imagebind.modality_trunks.thermal.blocks.8.attn.out_proj.weight', 'imagebind.modality_trunks.thermal.blocks.8.attn.out_proj.bias', 'imagebind.modality_trunks.thermal.blocks.8.norm_1.weight', 'imagebind.modality_trunks.thermal.blocks.8.norm_1.bias', 'imagebind.modality_trunks.thermal.blocks.8.mlp.fc1.weight', 'imagebind.modality_trunks.thermal.blocks.8.mlp.fc1.bias', 'imagebind.modality_trunks.thermal.blocks.8.mlp.fc2.weight', 'imagebind.modality_trunks.thermal.blocks.8.mlp.fc2.bias', 'imagebind.modality_trunks.thermal.blocks.8.norm_2.weight', 'imagebind.modality_trunks.thermal.blocks.8.norm_2.bias', 'imagebind.modality_trunks.thermal.blocks.9.attn.in_proj_weight', 'imagebind.modality_trunks.thermal.blocks.9.attn.in_proj_bias', 'imagebind.modality_trunks.thermal.blocks.9.attn.bias_k', 'imagebind.modality_trunks.thermal.blocks.9.attn.bias_v', 'imagebind.modality_trunks.thermal.blocks.9.attn.out_proj.weight', 'imagebind.modality_trunks.thermal.blocks.9.attn.out_proj.bias', 'imagebind.modality_trunks.thermal.blocks.9.norm_1.weight', 'imagebind.modality_trunks.thermal.blocks.9.norm_1.bias', 'imagebind.modality_trunks.thermal.blocks.9.mlp.fc1.weight', 'imagebind.modality_trunks.thermal.blocks.9.mlp.fc1.bias', 'imagebind.modality_trunks.thermal.blocks.9.mlp.fc2.weight', 'imagebind.modality_trunks.thermal.blocks.9.mlp.fc2.bias', 'imagebind.modality_trunks.thermal.blocks.9.norm_2.weight', 'imagebind.modality_trunks.thermal.blocks.9.norm_2.bias', 'imagebind.modality_trunks.thermal.blocks.10.attn.in_proj_weight', 'imagebind.modality_trunks.thermal.blocks.10.attn.in_proj_bias', 'imagebind.modality_trunks.thermal.blocks.10.attn.bias_k', 'imagebind.modality_trunks.thermal.blocks.10.attn.bias_v', 'imagebind.modality_trunks.thermal.blocks.10.attn.out_proj.weight', 'imagebind.modality_trunks.thermal.blocks.10.attn.out_proj.bias', 'imagebind.modality_trunks.thermal.blocks.10.norm_1.weight', 'imagebind.modality_trunks.thermal.blocks.10.norm_1.bias', 'imagebind.modality_trunks.thermal.blocks.10.mlp.fc1.weight', 'imagebind.modality_trunks.thermal.blocks.10.mlp.fc1.bias', 'imagebind.modality_trunks.thermal.blocks.10.mlp.fc2.weight', 'imagebind.modality_trunks.thermal.blocks.10.mlp.fc2.bias', 'imagebind.modality_trunks.thermal.blocks.10.norm_2.weight', 'imagebind.modality_trunks.thermal.blocks.10.norm_2.bias', 'imagebind.modality_trunks.thermal.blocks.11.attn.in_proj_weight', 'imagebind.modality_trunks.thermal.blocks.11.attn.in_proj_bias', 'imagebind.modality_trunks.thermal.blocks.11.attn.bias_k', 'imagebind.modality_trunks.thermal.blocks.11.attn.bias_v', 'imagebind.modality_trunks.thermal.blocks.11.attn.out_proj.weight', 'imagebind.modality_trunks.thermal.blocks.11.attn.out_proj.bias', 'imagebind.modality_trunks.thermal.blocks.11.norm_1.weight', 'imagebind.modality_trunks.thermal.blocks.11.norm_1.bias', 'imagebind.modality_trunks.thermal.blocks.11.mlp.fc1.weight', 'imagebind.modality_trunks.thermal.blocks.11.mlp.fc1.bias', 'imagebind.modality_trunks.thermal.blocks.11.mlp.fc2.weight', 'imagebind.modality_trunks.thermal.blocks.11.mlp.fc2.bias', 'imagebind.modality_trunks.thermal.blocks.11.norm_2.weight', 'imagebind.modality_trunks.thermal.blocks.11.norm_2.bias', 'imagebind.modality_trunks.imu.blocks.0.attn.in_proj_weight', 'imagebind.modality_trunks.imu.blocks.0.attn.in_proj_bias', 'imagebind.modality_trunks.imu.blocks.0.attn.bias_k', 'imagebind.modality_trunks.imu.blocks.0.attn.bias_v', 'imagebind.modality_trunks.imu.blocks.0.attn.out_proj.weight', 'imagebind.modality_trunks.imu.blocks.0.attn.out_proj.bias', 'imagebind.modality_trunks.imu.blocks.0.norm_1.weight', 'imagebind.modality_trunks.imu.blocks.0.norm_1.bias', 'imagebind.modality_trunks.imu.blocks.0.mlp.fc1.weight', 'imagebind.modality_trunks.imu.blocks.0.mlp.fc1.bias', 'imagebind.modality_trunks.imu.blocks.0.mlp.fc2.weight', 'imagebind.modality_trunks.imu.blocks.0.mlp.fc2.bias', 'imagebind.modality_trunks.imu.blocks.0.norm_2.weight', 'imagebind.modality_trunks.imu.blocks.0.norm_2.bias', 'imagebind.modality_trunks.imu.blocks.1.attn.in_proj_weight', 'imagebind.modality_trunks.imu.blocks.1.attn.in_proj_bias', 'imagebind.modality_trunks.imu.blocks.1.attn.bias_k', 'imagebind.modality_trunks.imu.blocks.1.attn.bias_v', 'imagebind.modality_trunks.imu.blocks.1.attn.out_proj.weight', 'imagebind.modality_trunks.imu.blocks.1.attn.out_proj.bias', 'imagebind.modality_trunks.imu.blocks.1.norm_1.weight', 'imagebind.modality_trunks.imu.blocks.1.norm_1.bias', 'imagebind.modality_trunks.imu.blocks.1.mlp.fc1.weight', 'imagebind.modality_trunks.imu.blocks.1.mlp.fc1.bias', 'imagebind.modality_trunks.imu.blocks.1.mlp.fc2.weight', 'imagebind.modality_trunks.imu.blocks.1.mlp.fc2.bias', 'imagebind.modality_trunks.imu.blocks.1.norm_2.weight', 'imagebind.modality_trunks.imu.blocks.1.norm_2.bias', 'imagebind.modality_trunks.imu.blocks.2.attn.in_proj_weight', 'imagebind.modality_trunks.imu.blocks.2.attn.in_proj_bias', 'imagebind.modality_trunks.imu.blocks.2.attn.bias_k', 'imagebind.modality_trunks.imu.blocks.2.attn.bias_v', 'imagebind.modality_trunks.imu.blocks.2.attn.out_proj.weight', 'imagebind.modality_trunks.imu.blocks.2.attn.out_proj.bias', 'imagebind.modality_trunks.imu.blocks.2.norm_1.weight', 'imagebind.modality_trunks.imu.blocks.2.norm_1.bias', 'imagebind.modality_trunks.imu.blocks.2.mlp.fc1.weight', 'imagebind.modality_trunks.imu.blocks.2.mlp.fc1.bias', 'imagebind.modality_trunks.imu.blocks.2.mlp.fc2.weight', 'imagebind.modality_trunks.imu.blocks.2.mlp.fc2.bias', 'imagebind.modality_trunks.imu.blocks.2.norm_2.weight', 'imagebind.modality_trunks.imu.blocks.2.norm_2.bias', 'imagebind.modality_trunks.imu.blocks.3.attn.in_proj_weight', 'imagebind.modality_trunks.imu.blocks.3.attn.in_proj_bias', 'imagebind.modality_trunks.imu.blocks.3.attn.bias_k', 'imagebind.modality_trunks.imu.blocks.3.attn.bias_v', 'imagebind.modality_trunks.imu.blocks.3.attn.out_proj.weight', 'imagebind.modality_trunks.imu.blocks.3.attn.out_proj.bias', 'imagebind.modality_trunks.imu.blocks.3.norm_1.weight', 'imagebind.modality_trunks.imu.blocks.3.norm_1.bias', 'imagebind.modality_trunks.imu.blocks.3.mlp.fc1.weight', 'imagebind.modality_trunks.imu.blocks.3.mlp.fc1.bias', 'imagebind.modality_trunks.imu.blocks.3.mlp.fc2.weight', 'imagebind.modality_trunks.imu.blocks.3.mlp.fc2.bias', 'imagebind.modality_trunks.imu.blocks.3.norm_2.weight', 'imagebind.modality_trunks.imu.blocks.3.norm_2.bias', 'imagebind.modality_trunks.imu.blocks.4.attn.in_proj_weight', 'imagebind.modality_trunks.imu.blocks.4.attn.in_proj_bias', 'imagebind.modality_trunks.imu.blocks.4.attn.bias_k', 'imagebind.modality_trunks.imu.blocks.4.attn.bias_v', 'imagebind.modality_trunks.imu.blocks.4.attn.out_proj.weight', 'imagebind.modality_trunks.imu.blocks.4.attn.out_proj.bias', 'imagebind.modality_trunks.imu.blocks.4.norm_1.weight', 'imagebind.modality_trunks.imu.blocks.4.norm_1.bias', 'imagebind.modality_trunks.imu.blocks.4.mlp.fc1.weight', 'imagebind.modality_trunks.imu.blocks.4.mlp.fc1.bias', 'imagebind.modality_trunks.imu.blocks.4.mlp.fc2.weight', 'imagebind.modality_trunks.imu.blocks.4.mlp.fc2.bias', 'imagebind.modality_trunks.imu.blocks.4.norm_2.weight', 'imagebind.modality_trunks.imu.blocks.4.norm_2.bias', 'imagebind.modality_trunks.imu.blocks.5.attn.in_proj_weight', 'imagebind.modality_trunks.imu.blocks.5.attn.in_proj_bias', 'imagebind.modality_trunks.imu.blocks.5.attn.bias_k', 'imagebind.modality_trunks.imu.blocks.5.attn.bias_v', 'imagebind.modality_trunks.imu.blocks.5.attn.out_proj.weight', 'imagebind.modality_trunks.imu.blocks.5.attn.out_proj.bias', 'imagebind.modality_trunks.imu.blocks.5.norm_1.weight', 'imagebind.modality_trunks.imu.blocks.5.norm_1.bias', 'imagebind.modality_trunks.imu.blocks.5.mlp.fc1.weight', 'imagebind.modality_trunks.imu.blocks.5.mlp.fc1.bias', 'imagebind.modality_trunks.imu.blocks.5.mlp.fc2.weight', 'imagebind.modality_trunks.imu.blocks.5.mlp.fc2.bias', 'imagebind.modality_trunks.imu.blocks.5.norm_2.weight', 'imagebind.modality_trunks.imu.blocks.5.norm_2.bias', 'imagebind.modality_heads.vision.0.weight', 'imagebind.modality_heads.vision.0.bias', 'imagebind.modality_heads.vision.2.weight', 'imagebind.modality_heads.text.proj.0.weight', 'imagebind.modality_heads.text.proj.0.bias', 'imagebind.modality_heads.text.proj.1.weight', 'imagebind.modality_heads.audio.0.weight', 'imagebind.modality_heads.audio.0.bias', 'imagebind.modality_heads.audio.2.weight', 'imagebind.modality_heads.depth.0.weight', 'imagebind.modality_heads.depth.0.bias', 'imagebind.modality_heads.depth.2.weight', 'imagebind.modality_heads.thermal.0.weight', 'imagebind.modality_heads.thermal.0.bias', 'imagebind.modality_heads.thermal.2.weight', 'imagebind.modality_heads.imu.0.weight', 'imagebind.modality_heads.imu.0.bias', 'imagebind.modality_heads.imu.3.weight', 'imagebind.modality_postprocessors.text.1.log_logit_scale', 'imagebind.modality_postprocessors.audio.1.log_logit_scale', 'imagebind.modality_postprocessors.depth.1.log_logit_scale', 'imagebind.modality_postprocessors.thermal.1.log_logit_scale', 'imagebind.modality_postprocessors.imu.1.log_logit_scale', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'classifier.weight', 'classifier.bias'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "checkpoint['state_dict'].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
