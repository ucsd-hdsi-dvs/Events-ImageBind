{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import torch\n",
    "from models import imagebind_model\n",
    "from models.imagebind_model import ModalityType\n",
    "\n",
    "text_list=[\"A dog.\", \"A car\", \"A bird\"]\n",
    "image_paths=[\".assets/dog_image.jpg\", \".assets/car_image.jpg\", \".assets/bird_image.jpg\"]\n",
    "audio_paths=[\".assets/dog_audio.wav\", \".assets/car_audio.wav\", \".assets/bird_audio.wav\"]\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Load data\n",
    "inputs = {\n",
    "    ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "    ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n",
    "    ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(inputs)\n",
    "\n",
    "print(\n",
    "    \"Vision x Text: \",\n",
    "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    ")\n",
    "print(\n",
    "    \"Audio x Text: \",\n",
    "    torch.softmax(embeddings[ModalityType.AUDIO] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    ")\n",
    "print(\n",
    "    \"Vision x Audio: \",\n",
    "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.AUDIO].T, dim=-1),\n",
    ")\n",
    "\n",
    "# Expected output:\n",
    "#\n",
    "# Vision x Text:\n",
    "# tensor([[9.9761e-01, 2.3694e-03, 1.8612e-05],\n",
    "#         [3.3836e-05, 9.9994e-01, 2.4118e-05],\n",
    "#         [4.7997e-05, 1.3496e-02, 9.8646e-01]])\n",
    "#\n",
    "# Audio x Text:\n",
    "# tensor([[1., 0., 0.],\n",
    "#         [0., 1., 0.],\n",
    "#         [0., 0., 1.]])\n",
    "#\n",
    "# Vision x Audio:\n",
    "# tensor([[0.8070, 0.1088, 0.0842],\n",
    "#         [0.1036, 0.7884, 0.1079],\n",
    "#         [0.0018, 0.0022, 0.9960]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.AUDIO].T, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.multimodal_preprocessors import PatchEmbedGeneric,PadIm2Video\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "class PatchEmbedGeneric(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchEmbed from Hydra\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, proj_stem, norm_layer: Optional[nn.Module] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        if len(proj_stem) > 1:\n",
    "            self.proj = nn.Sequential(*proj_stem)\n",
    "        else:\n",
    "            # Special case to be able to load pre-trained models that were\n",
    "            # trained with a standard stem\n",
    "            self.proj = proj_stem[0]\n",
    "        self.norm_layer = norm_layer\n",
    "\n",
    "    def get_patch_layout(self, img_size):\n",
    "        with torch.no_grad():\n",
    "            dummy_img = torch.zeros(\n",
    "                [\n",
    "                    1,\n",
    "                ]\n",
    "                + img_size\n",
    "            )\n",
    "            print(dummy_img.shape)\n",
    "            dummy_out = self.proj(dummy_img)\n",
    "        print(dummy_out.shape)\n",
    "        embed_dim = dummy_out.shape[1]\n",
    "        patches_layout = tuple(dummy_out.shape[2:])\n",
    "        num_patches = np.prod(patches_layout)\n",
    "        return patches_layout, num_patches, embed_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = self.proj(x)\n",
    "        print(x.shape)\n",
    "        # B C (T) H W -> B (T)HW C\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        print(x.shape)\n",
    "        if self.norm_layer is not None:\n",
    "            x = self.norm_layer(x)\n",
    "        return x\n",
    "    \n",
    "kernel_size=(2, 14, 14)\n",
    "vision_embed_dim=1024\n",
    "proj_stem=[\n",
    "                PadIm2Video(pad_type=\"repeat\", ntimes=2),\n",
    "                nn.Conv3d(\n",
    "                    in_channels=3,\n",
    "                    kernel_size=kernel_size,\n",
    "                    out_channels=vision_embed_dim,\n",
    "                    stride=kernel_size,\n",
    "                    bias=False,\n",
    "                )\n",
    "]\n",
    "PatchEmbedGeneric(proj_stem,None).get_patch_layout([3, 2,224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.helpers import VerboseNNModule\n",
    "from typing import Tuple, Optional, Callable\n",
    "from models.helpers import (EinOpsRearrange, LearnableLogitScaling, Normalize,\n",
    "                            SelectElement, SelectEOSAndProject)\n",
    "from models.multimodal_preprocessors import (AudioPreprocessor,\n",
    "                                             IMUPreprocessor, PadIm2Video,\n",
    "                                             PatchEmbedGeneric,\n",
    "                                             RGBDTPreprocessor,\n",
    "                                             SpatioTemporalPosEmbeddingHelper,\n",
    "                                             TextPreprocessor,\n",
    "                                             ThermalPreprocessor)\n",
    "from models.transformer import MultiheadAttention, SimpleTransformer\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from functools import partial\n",
    "from types import SimpleNamespace\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RGBDTPreprocessor(VerboseNNModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rgbt_stem: PatchEmbedGeneric,\n",
    "        depth_stem: Optional[PatchEmbedGeneric],\n",
    "        img_size: Tuple = (3, 224, 224),\n",
    "        num_cls_tokens: int = 1,\n",
    "        pos_embed_fn: Optional[Callable] = None,\n",
    "        use_type_embed: bool = False,\n",
    "        init_param_style: str = \"openclip\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        stem = rgbt_stem if rgbt_stem is not None else depth_stem\n",
    "        (\n",
    "            self.patches_layout,\n",
    "            self.num_patches,\n",
    "            self.embed_dim,\n",
    "        ) = stem.get_patch_layout(img_size)\n",
    "        self.rgbt_stem = rgbt_stem\n",
    "        self.depth_stem = depth_stem\n",
    "        self.use_pos_embed = pos_embed_fn is not None\n",
    "        self.use_type_embed = use_type_embed\n",
    "        self.num_cls_tokens = num_cls_tokens\n",
    "\n",
    "        if self.use_pos_embed:\n",
    "            self.pos_embedding_helper = pos_embed_fn(\n",
    "                patches_layout=self.patches_layout,\n",
    "                num_cls_tokens=num_cls_tokens,\n",
    "                num_patches=self.num_patches,\n",
    "                embed_dim=self.embed_dim,\n",
    "            )\n",
    "        if self.num_cls_tokens > 0:\n",
    "            self.cls_token = nn.Parameter(\n",
    "                torch.zeros(1, self.num_cls_tokens, self.embed_dim)\n",
    "            )\n",
    "        if self.use_type_embed:\n",
    "            self.type_embed = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "\n",
    "        self.init_parameters(init_param_style)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_parameters(self, init_param_style):\n",
    "        if init_param_style == \"openclip\":\n",
    "            # OpenCLIP style initialization\n",
    "            scale = self.embed_dim**-0.5\n",
    "            if self.use_pos_embed:\n",
    "                nn.init.normal_(self.pos_embedding_helper.pos_embed)\n",
    "                self.pos_embedding_helper.pos_embed *= scale\n",
    "\n",
    "            if self.num_cls_tokens > 0:\n",
    "                nn.init.normal_(self.cls_token)\n",
    "                self.cls_token *= scale\n",
    "        elif init_param_style == \"vit\":\n",
    "            self.cls_token.data.fill_(0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown init {init_param_style}\")\n",
    "\n",
    "        if self.use_type_embed:\n",
    "            nn.init.normal_(self.type_embed)\n",
    "\n",
    "    def tokenize_input_and_cls_pos(self, input, stem, mask):\n",
    "        # tokens is of shape B x L x D\n",
    "        tokens = stem(input)\n",
    "        assert tokens.ndim == 3\n",
    "        assert tokens.shape[2] == self.embed_dim\n",
    "        B = tokens.shape[0]\n",
    "        if self.num_cls_tokens > 0:\n",
    "            class_tokens = self.cls_token.expand(\n",
    "                B, -1, -1\n",
    "            )  # stole class_tokens impl from Phil Wang, thanks\n",
    "            tokens = torch.cat((class_tokens, tokens), dim=1)\n",
    "        if self.use_pos_embed:\n",
    "            pos_embed = self.pos_embedding_helper.get_pos_embedding(input, tokens)\n",
    "            tokens = tokens + pos_embed\n",
    "        if self.use_type_embed:\n",
    "            tokens = tokens + self.type_embed.expand(B, -1, -1)\n",
    "        return tokens\n",
    "\n",
    "    def forward(self, vision=None, depth=None, patch_mask=None):\n",
    "        if patch_mask is not None:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        if vision is not None:\n",
    "            vision_tokens = self.tokenize_input_and_cls_pos(\n",
    "                vision, self.rgbt_stem, patch_mask\n",
    "            )\n",
    "\n",
    "        if depth is not None:\n",
    "            depth_tokens = self.tokenize_input_and_cls_pos(\n",
    "                depth, self.depth_stem, patch_mask\n",
    "            )\n",
    "\n",
    "        # aggregate tokens\n",
    "        if vision is not None and depth is not None:\n",
    "            final_tokens = vision_tokens + depth_tokens\n",
    "        else:\n",
    "            final_tokens = vision_tokens if vision is not None else depth_tokens\n",
    "        return_dict = {\n",
    "            \"trunk\": {\n",
    "                \"tokens\": final_tokens,\n",
    "            },\n",
    "            \"head\": {},\n",
    "        }\n",
    "        return return_dict\n",
    "\n",
    "rgbt_stem = PatchEmbedGeneric(\n",
    "            proj_stem=[\n",
    "                PadIm2Video(pad_type=\"repeat\", ntimes=2),\n",
    "                nn.Conv3d(\n",
    "                    in_channels=3,\n",
    "                    kernel_size=kernel_size,\n",
    "                    out_channels=vision_embed_dim,\n",
    "                    stride=kernel_size,\n",
    "                    bias=False,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "rgbt_preprocessor = RGBDTPreprocessor(\n",
    "            img_size=[3, 2, 224, 224],\n",
    "            num_cls_tokens=1,\n",
    "            pos_embed_fn=partial(SpatioTemporalPosEmbeddingHelper, learnable=True),\n",
    "            rgbt_stem=rgbt_stem,\n",
    "            depth_stem=None,\n",
    "        )\n",
    "\n",
    "rgbt_preprocessor(dummy_img[0])['trunk']['tokens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/imagebind/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models.imagebind_model import ImageBindModel\n",
    "imageBind=ImageBindModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([1, 3, 224, 224])\n",
      "trunk_inputs shape torch.Size([1, 257, 1024])\n",
      "after trunk shape torch.Size([1, 257, 1024])\n",
      "after head shape torch.Size([1, 768])\n",
      "postprocessor shape torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dummy_img = torch.zeros(\n",
    "                [\n",
    "                    1,\n",
    "                ]\n",
    "                + [3, 224, 224]\n",
    "            )\n",
    "imageBind.layer_shapes(dummy_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "path='.checkpoints/imagebind_huge.pth'\n",
    "state_dict = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modality_preprocessors.thermal.cls_token',\n",
       " 'modality_preprocessors.thermal.rgbt_stem.proj.weight',\n",
       " 'modality_preprocessors.thermal.rgbt_stem.norm_layer.weight',\n",
       " 'modality_preprocessors.thermal.rgbt_stem.norm_layer.bias',\n",
       " 'modality_preprocessors.thermal.pos_embedding_helper.pos_embed',\n",
       " 'modality_trunks.thermal.blocks.0.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.0.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.0.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.0.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.0.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.0.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.0.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.0.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.0.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.0.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.0.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.0.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.0.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.0.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.1.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.1.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.1.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.1.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.1.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.1.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.1.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.1.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.1.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.1.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.1.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.1.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.1.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.1.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.2.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.2.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.2.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.2.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.2.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.2.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.2.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.2.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.2.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.2.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.2.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.2.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.2.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.2.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.3.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.3.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.3.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.3.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.3.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.3.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.3.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.3.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.3.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.3.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.3.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.3.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.3.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.3.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.4.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.4.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.4.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.4.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.4.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.4.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.4.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.4.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.4.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.4.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.4.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.4.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.4.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.4.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.5.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.5.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.5.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.5.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.5.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.5.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.5.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.5.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.5.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.5.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.5.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.5.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.5.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.5.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.6.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.6.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.6.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.6.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.6.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.6.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.6.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.6.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.6.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.6.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.6.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.6.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.6.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.6.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.7.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.7.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.7.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.7.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.7.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.7.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.7.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.7.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.7.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.7.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.7.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.7.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.7.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.7.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.8.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.8.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.8.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.8.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.8.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.8.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.8.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.8.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.8.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.8.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.8.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.8.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.8.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.8.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.9.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.9.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.9.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.9.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.9.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.9.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.9.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.9.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.9.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.9.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.9.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.9.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.9.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.9.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.10.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.10.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.10.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.10.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.10.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.10.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.10.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.10.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.10.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.10.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.10.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.10.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.10.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.10.norm_2.bias',\n",
       " 'modality_trunks.thermal.blocks.11.attn.in_proj_weight',\n",
       " 'modality_trunks.thermal.blocks.11.attn.in_proj_bias',\n",
       " 'modality_trunks.thermal.blocks.11.attn.bias_k',\n",
       " 'modality_trunks.thermal.blocks.11.attn.bias_v',\n",
       " 'modality_trunks.thermal.blocks.11.attn.out_proj.weight',\n",
       " 'modality_trunks.thermal.blocks.11.attn.out_proj.bias',\n",
       " 'modality_trunks.thermal.blocks.11.norm_1.weight',\n",
       " 'modality_trunks.thermal.blocks.11.norm_1.bias',\n",
       " 'modality_trunks.thermal.blocks.11.mlp.fc1.weight',\n",
       " 'modality_trunks.thermal.blocks.11.mlp.fc1.bias',\n",
       " 'modality_trunks.thermal.blocks.11.mlp.fc2.weight',\n",
       " 'modality_trunks.thermal.blocks.11.mlp.fc2.bias',\n",
       " 'modality_trunks.thermal.blocks.11.norm_2.weight',\n",
       " 'modality_trunks.thermal.blocks.11.norm_2.bias',\n",
       " 'modality_heads.thermal.0.weight',\n",
       " 'modality_heads.thermal.0.bias',\n",
       " 'modality_heads.thermal.2.weight',\n",
       " 'modality_postprocessors.thermal.1.log_logit_scale']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[keys for keys in state_dict.keys() if 'thermal' in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.events import EventModel\n",
    "e=EventModel()\n",
    "e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
